import java.io.File;
import java.util.ArrayList;

import cc.mallet.util.*;
import cc.mallet.types.*;
import cc.mallet.pipe.*;
import cc.mallet.pipe.iterator.*;
import cc.mallet.topics.*;

import java.util.*;
import java.util.logging.Level;
import java.util.regex.*;
import java.io.*;

/*
 * This class manages a Collection of books structured in a tree of BookNode
 * and includes the topic model object and the methods for run the modeling over
 * the collection and propagate the topic assignment along the tree structures
 * 
 * @author Julio Guerra
 */
public class Collection extends BookNode {

	// this is the model object. It has methods for running LDA (see Mallet API documentation)
	public ParallelTopicModel model_LDA;
	public HierarchicalLDA model_hLDA;
	public HLDAUtil hlda_util;
	
	public InstanceList training_instances;
	public InstanceList test_instances;
	
	public double[] topic_weights;
	
	public Collection(String title, String id){
		super(id,title,null);
		model_LDA = null;
		model_hLDA = null;
	}
	
	public void setTopicWeights(double[] tw){
		this.topic_weights = tw;
	}
	public double getTopicWeight(int i){
		if (i<topic_weights.length) return topic_weights[i];
		return 0.0;
	}
	
	// this attribute represents the topic document frequency and 
	// will be filled with the the number of nodes in the books having the 
	// each topic (topic_df[0] correspond to topic 0) with a probability higher than
	// a threshold number
	private int[] topic_df;
	public int[] getTopicDf(){
		return this.topic_df;
	}
	
	// This method computes the topic document frequency counting only the documents
	// having a probability higher than min_prob in the correspondent topic
	private void computeTopicDF(double min_prob){
		ArrayList<TopicAssignment> data = model_LDA.getData();
        int n = model_LDA.getNumTopics();
        
        this.topic_df = new int[n];
        
        for(int i=0;i<data.size();i++){
        	double[] probabilities = this.model_LDA.getTopicProbabilities(i);
			for(int j=0;j<n;j++){
				if (probabilities[j]>min_prob) topic_df[j]++;
			}			
		}
        
	}
	
	// this array will be filled with the book indexes (book 1, 2, and so on)
	// for each document in the collection, as ordered. Each element in the array correspond to
	// a document of the collection (each BookNode) in the order it is generated by
	// the method BookNode
	private int[] book_indexes;
	public int[] getBookIndexes(){
		return book_indexes;
	}
	
	// fill the book_indexes array
	public void computeBookIndexes(){
		// Build an array that labels with the book number corresponding to each node
		BookNode[] array_nodes = this.toArray();
		int s = array_nodes.length;
		book_indexes = new int[s];
		
		// For each book, get the book node size
		int book_index = 0;
		book_indexes[0] = book_index; // the first node is the collection
		int n = 1;
		for (BookNode book : children){
			book_index++;
			int bsize = book.size();
			for (int k=n;k<n+bsize;k++){
				book_indexes[k] = book_index;
			}
			n += bsize;
		}
	}	
	
	// returns true if two nodes are in the same book (have the same index in the book_indexes array)
	public boolean areInTheSameBook(BookNode nodeA, BookNode nodeB){
		ArrayList<BookNode> array_nodes = new ArrayList<BookNode>();
		this.toArrayList(array_nodes);
		int indexA = array_nodes.indexOf(nodeA);
		int indexB = array_nodes.indexOf(nodeB);
		return book_indexes[indexA] == book_indexes[indexB];
	}
	
	// get the book number for a node from the book_indexes array
	public int getBookNumber(BookNode node){
		ArrayList<BookNode> array_nodes = new ArrayList<BookNode>();
		this.toArrayList(array_nodes);		
		int index = array_nodes.indexOf(node);
		return book_indexes[index];
	}
	


	// generates a string having one line for each document (node) in the
	// collection. Considers only nodes that have content (text)
	public String generateLDAInputString(boolean[] usingbooks){
		String r = "";
		int i=0;
		for (BookNode child : children){
			if (usingbooks[i]) r += child.treeToLDAInputString(true);
			i++;
		}		
		return r;
	}
	
	// Aggregates content for each chapter. Each chapter will have a text aggregating 
	// it's own text with the text of all children in the branch (all levesl. 
	// The process is repeated recursively in each child
	public void aggregateContent(){
		for (BookNode child : children){
			for (BookNode chapter : child.children){
				chapter.aggregateContent();
			}
		}
	}

	// This method set a uniform distribution (1/N_topics) to every node in the collection
	// and is called after each run of the topic modeling estimation.
	// This prevents old topic distributions that remains in parent (propagated nodes)
	public void resetTopicAssignment(int n_topics, boolean use_uniform_dist){
		BookNode[] array_nodes = this.toArray();
		int s = array_nodes.length;
		if (!use_uniform_dist){
			for (int i=0; i<s;i++){
				array_nodes[i].setTopicProbabilities(new double[n_topics]);
				array_nodes[i].setTfidf(new double[n_topics]);
			}
			
		}else{
			for (int i=0; i<s;i++){
				array_nodes[i].setUniformTopicDistribution(n_topics);
			}
		}
	}
	
	
	private InstanceList prepareInstanceListForLDA(String content) throws IOException{
		// Code from Mallet example: http://mallet.cs.umass.edu/topics-devel.php
		
		// Begin by importing documents from text to feature sequences
        ArrayList<Pipe> pipeList = new ArrayList<Pipe>();

        // Pipes: lowercase, tokenize, remove stopwords, map to features
        pipeList.add( new CharSequenceLowercase() );
        pipeList.add( new CharSequence2TokenSequence(Pattern.compile("\\p{L}[\\p{L}\\p{P}]+\\p{L}")) );
        pipeList.add( new TokenSequenceRemoveStopwords(new File("stoplists/en.txt"), "UTF-8", false, false, false) );
        // an extra stop word list is considered for the domain
        pipeList.add( new TokenSequenceRemoveStopwords(new File("stoplists/en_extra.txt"), "UTF-8", false, false, false) );
        pipeList.add( new TokenSequence2FeatureSequence() );

        InstanceList instances = new InstanceList (new SerialPipes(pipeList));
        
        Reader contentReader = new StringReader(content);
        instances.addThruPipe(new CsvIterator (contentReader, Pattern.compile("^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$"),
                                               3, 2, 1)); // data, label, name fields
        return instances;
	}
	
	// This method prepares the topic model engine to run with the String content. 
	// Set up is standard for Mallet topic modeling
	// Also, the some text processing is done: filtering special characters (only letters and numbers are kept)
	// and stop words removal
	
	private ParallelTopicModel prepareLDAModel(String content, int num_topics, int iterations, double alpha, double beta, boolean symmetric_alpha) throws IOException{
		ParallelTopicModel.logger.setLevel(Level.OFF);
        
		InstanceList instances = prepareInstanceListForLDA(content);
        this.training_instances = instances;
        // the model object is created, but having no topic estimation yet. 
        ParallelTopicModel model = new ParallelTopicModel(num_topics, alpha, beta);
        model.addInstances(instances);
        model.setNumThreads(1);
        model.setSymmetricAlpha(symmetric_alpha);
        model.setTopicDisplay(500, 11);
        model.setBurninPeriod(100);
        model.setNumIterations(iterations);
        return model;
	}
	
	// this method runs the topic model estimation
	
	public void runLDA(String content, int num_topics, int iterations, double alpha, double beta, boolean symmetric_alpha, double min_topic_prob) throws IOException{
		// 1. reset the topic assignment preventing old topic distribution in parent nodes
		this.resetTopicAssignment(num_topics, true);
		// 2. prepare the model (see method above)
		this.model_LDA = prepareLDAModel( content, num_topics, iterations, alpha, beta, symmetric_alpha);
		
		// 3. run the estimation
        this.model_LDA.estimate();
        
        // 4. fill the topic_df array with the document frequencies of each topic
        //    only counts documents having higher probability than min_topic_prob for each topic
        this.computeTopicDF(min_topic_prob);
        
        // 5. Get the topic distribution for each doc and save the data in the corresponding BookNode
        ArrayList<TopicAssignment> data = model_LDA.getData();        
        // the number of processed docs
        int n = data.size();
        // For each topic assignment (each document), get the topic probabilities and store in
        // the corresponding node
        int i = 0;
        for(TopicAssignment topic_assign : data){
			// get the document instance
        	Instance doc_instance = topic_assign.instance;
			// the id of the document
        	String doc = doc_instance.getName().toString();
        	// find the node with this id
			BookNode node = this.searchNodeById(doc);
			// assign the probability vector to the node
			double[] probabilities = this.model_LDA.getTopicProbabilities(i);
			
			node.setTopicProbabilities(probabilities);
			node.setProcessedTextLength(data.get(i).topicSequence.getLength());
			
			// call this method for compute the tfidf of each topic in the node
			node.computeTFIDF(topic_df, n);
			
			i++;
		}
        
        // fill the book indexes array, used in next steps
        this.computeBookIndexes();
	}
	
	public void runHLDA(String content, int num_levels, int iterations, double alpha, double gamma, double eta) throws IOException{
		
		this.training_instances = prepareInstanceListForLDA(content);
       
        model_hLDA = new HierarchicalLDA();
		// Set hyperparameters

		model_hLDA.setAlpha(alpha);
		model_hLDA.setGamma(gamma);
		model_hLDA.setEta(eta);
		
		// Display preferences
		model_hLDA.setTopicDisplay(iterations*2,10);
		model_hLDA.setProgressDisplay(true);
		

		// Initialize random number generator

		Randoms random = new Randoms();

		// Initialize and start the sampler

		this.model_hLDA.initialize(this.training_instances, this.training_instances, num_levels, random);
		
		this.model_hLDA.estimate(iterations);
		HierarchicalLDAInferencer inferencer = new HierarchicalLDAInferencer(this.model_hLDA);
		this.hlda_util = new HLDAUtil(inferencer);
		
		int ntopics = this.hlda_util.countTopics();
		int ndocs = this.training_instances.size();
		this.resetTopicAssignment(ntopics, false);	
		
		this.topic_df = new int[ntopics];
		BookNode[] processed_nodes = new BookNode[ndocs];
		for (int i=0;i<ndocs;i++){
			Instance doc_instance = this.training_instances.get(i);
			double[] probabilities = this.hlda_util.getTopicDistribution(i);
			
			for (int j=0;j<probabilities.length;j++){
				if (probabilities[j]>0.001) this.topic_df[j]++;
			}
			String doc = doc_instance.getName().toString();
        	
			BookNode node = this.searchNodeById(doc);
			
			node.setTopicProbabilities(probabilities);
			
			node.setProcessedTextLength(((FeatureSequence)doc_instance.getData()).size());
			processed_nodes[i] = node;
			node.computeTFIDF(this.topic_df, ndocs);
		}
		for(int i=0;i<ndocs;i++){
			processed_nodes[i].computeTFIDF(this.topic_df, ndocs);
		}
		
		//
		this.computeBookIndexes();
		
	}

	// @@@ DEPRECATED - not used
	// 
	public double[][] mappingByTopicProbabilities(){
	
		BookNode[] array_nodes = this.toArray();
		int s = array_nodes.length;
		double[][] map_matrix = new double[s][s];
				
		
		for (int i=0; i<s-1;i++){
			//System.out.println(array_nodes[i].getDocId()+" "+array_nodes[i].getTitle());
			BookNode nodeA = array_nodes[i];
			
			for (int j=i+1; j<s;j++){
				BookNode nodeB = array_nodes[j];
				
				map_matrix[i][j] = nodeA.computeSimilarity(nodeB, "COSINE",false, this.topic_weights, true,null);
				map_matrix[j][i] = map_matrix[i][j];
			}
			
		}
		
		return map_matrix;
	}
	
	
	// returns a topics X words array with the topic distribution over words
	public double[][] getTopicWordDistributionLDA(){
		int numTypes = this.model_LDA.numTypes;
		int numTopics = this.model_LDA.numTopics;
		double[][] distribution = new double[numTopics][numTypes];
		double sum = 0.0;
		for (int topic = 0; topic < numTopics; topic++) {
			sum = 0.0;
			for (int type = 0; type < numTypes; type++) {

				int[] topicCounts = this.model_LDA.typeTopicCounts[type];
				
				double weight = this.model_LDA.beta;
				//double weight = 0.0;

				int index = 0;
				while (index < topicCounts.length && topicCounts[index] > 0) {

					int currentTopic = topicCounts[index] & this.model_LDA.topicMask;	
					
					if (currentTopic == topic) {
						weight += topicCounts[index] >> this.model_LDA.topicBits;
						break;
					}

					index++;
				}
				distribution[topic][type] = weight;
				sum += weight;
				//System.out.println(topic + "\t" + alphabet.lookupObject(type) + "\t" + weight);

			}
			// convert into probabilities
			for (int type = 0; type < numTypes; type++) {
				distribution[topic][type] = distribution[topic][type]/sum;
			}
		}
		return distribution;
	}
	
	
	public void indexBookLDA(int booknumber) throws IOException{
		BookNode book = this.children.get(booknumber-1);
		//System.out.println("indexing book "+booknumber+" : "+book.getDocId());
		String content = book.treeToLDAInputString(true);
		//System.out.println(content);
		
		//InstanceList instances = prepareInstanceListForLDA(content);
		InstanceList instances = new InstanceList (training_instances.getPipe());
        
        Reader contentReader = new StringReader(content);
        instances.addThruPipe(new CsvIterator (contentReader, Pattern.compile("^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$"),3, 2, 1)); // data, label, name fields
		TopicInferencer inferencer = model_LDA.getInferencer();
		int n = instances.size();
		for(Instance instance: instances){
			double[] testProbabilities = inferencer.getSampledDistribution(instance, 2000, 500, 500);
			String doc = instance.getName().toString();
        	// find the node with this id
			BookNode node = this.searchNodeById(doc);
			//System.out.println("Setting probabilities of node "+node.getDocId());
			node.setTopicProbabilities(testProbabilities);
			
			//node.setProcessedTextLength(node.getText().split(" ").length);
			node.setProcessedTextLength(((FeatureSequence)instance.getData()).size());
			node.computeTFIDF(topic_df, n);
		}
	}
	
	public void indexBookHLDA(int booknumber) throws IOException{
		BookNode book = this.children.get(booknumber-1);
		//System.out.println("indexing book "+booknumber+" : "+book.getDocId());
		String content = book.treeToLDAInputString(true);
		//System.out.println(content);
		
		//InstanceList instances = prepareInstanceListForLDA(content);
		this.test_instances = new InstanceList (training_instances.getPipe());
        
        Reader contentReader = new StringReader(content);
        this.test_instances.addThruPipe(new CsvIterator (contentReader, Pattern.compile("^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$"),3, 2, 1)); // data, label, name fields
		
		int n = this.test_instances.size();
		for(Instance doc_instance: this.test_instances){
			double[] testProbabilities = this.hlda_util.getSampledDistribution(doc_instance, 3000, 1000, 1000);
			String doc = doc_instance.getName().toString();
        	// find the node with this id
			BookNode node = this.searchNodeById(doc);
			//System.out.println("Setting probabilities of node "+node.getDocId());
			node.setTopicProbabilities(testProbabilities);
			
			node.setProcessedTextLength(((FeatureSequence)doc_instance.getData()).size());
			node.computeTFIDF(topic_df, n);
		}
	}
	
	public void cutLowProbabilities(double threshold){
		BookNode[] array_nodes = this.toArray();
		int s = array_nodes.length;
		for (int i=0; i<s;i++){
			double[] probs = array_nodes[i].getTopicProbabilities();
			for (int j=0;j<probs.length;j++){
				if (probs[j]<threshold) probs[j] = 0.0001;
			}
		}
		
	}
	
	public double[][] getBookSimilarities(String method){
		int n = this.children.size();
		double[][] similarities = new double[n][n];
		for(int i=0;i<n;i++){
			BookNode child1 = this.children.get(i);
			
			for (int j=i;j<n;j++){
				
				BookNode child2 = this.children.get(j);
				if (method.equalsIgnoreCase("cosine")){
					similarities[i][j] = this.cosineSimilarity(child1.getTopicProbabilities(), child2.getTopicProbabilities(), this.topic_weights);	
				}else{
					similarities[i][j] = this.KLDivergenceSimilarity(child1.getTopicProbabilities(), child2.getTopicProbabilities(), this.topic_weights);
					//System.out.println(" between BOOK "+(i+1)+" and BOOK "+(j+1)+" : "+similarities[i][j]);
				}
				
				similarities[j][i] = similarities[i][j];
				
			}
			
		}
		return similarities;
	}
}
