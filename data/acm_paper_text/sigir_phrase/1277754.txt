robust-test collections for retrieval-evaluation low-cost methods for acquiring relevance-judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments . while these judgments are very useful for a one-time evaluation , it is not clear that they can be trusted when re-used to evaluate new systems . in this work , we formally define what it means for judgments to be reusable : the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance-judgments . we then present a method for augmenting a set of relevance-judgments with relevance estimates that require no additional assessor effort . using this method practically guarantees reusability : with as few as five judgments per topic taken from only two systems , we can reliably evaluate a larger set of ten systems . even the smallest sets of judgments can be useful for evaluation of new systems .