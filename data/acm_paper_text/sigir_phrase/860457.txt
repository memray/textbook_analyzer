using asymmetric distributions to improve text-classifier probability-estimates text classifiers that give probability-estimates are more readily applicable in a variety of scenarios . for example , rather than choosing one set decision threshold , they can be used in a bayesian risk-model to issue a run-time decision which minimizes a user-specified cost-function dynamically chosen at prediction time . however , the quality of the probability estimates is crucial . we review a variety of standard approaches to converting scores (and poor probability-estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score-distribution for the `` extremely irrelevant '' , `` hard to discriminate '' , and `` obviously relevant '' items are often significantly different . finally , we analyze the experimental performance of these models over the outputs of two text classifiers . the analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility) , computationally efficient , and empirically preferable .