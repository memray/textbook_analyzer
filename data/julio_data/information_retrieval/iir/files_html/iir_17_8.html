<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Implementation notes</title> 
  <meta name="description" content="Implementation notes" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="references-and-further-reading-17.html" /> 
  <link rel="previous" href="cluster-labeling-1.html" /> 
  <link rel="up" href="hierarchical-clustering-1.html" /> 
  <link rel="next" href="references-and-further-reading-17.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html4449" href="references-and-further-reading-17.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4443" href="hierarchical-clustering-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4437" href="cluster-labeling-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4445" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4447" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4450" href="references-and-further-reading-17.html">References and further reading</a> 
  <b> Up:</b> 
  <a name="tex2html4444" href="hierarchical-clustering-1.html">Hierarchical clustering</a> 
  <b> Previous:</b> 
  <a name="tex2html4438" href="cluster-labeling-1.html">Cluster labeling</a> &nbsp; 
  <b> <a name="tex2html4446" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4448" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION002280000000000000000"></a> <a name="sec:implnotes"></a> <a name="p:implnotes"></a> <br /> Implementation notes </h1> Most problems that require the computation of a large number of dot products benefit from an inverted index. This is also the case for HAC clustering. Computational savings due to the inverted index are large if there are many zero similarities - either because many documents do not share any terms or because an aggressive stop list is used. 
  <p> In low dimensions, more aggressive optimizations are possible that make the computation of most pairwise similarities unnecessary (Exercise <a href="exercises-4.html#ex:lowdim">17.10</a> ). However, no such algorithms are known in higher dimensions. We encountered the same problem in kNN classification (see Section <a href="references-and-further-reading-14.html#sec:vclassfurther">14.7</a> , page <a href="references-and-further-reading-14.html#p:vclassfurther">14.7</a> ). </p> 
  <p> When using GAAC on a large document set in high dimensions, we have to take care to avoid dense centroids. For dense centroids, clustering can take time 
   <!-- MATH
 $\Theta(MN^2\log N )$
 --> <img width="109" height="36" align="MIDDLE" border="0" src="img1657.png" alt="$\Theta(MN^2\log N )$" /> where <img width="20" height="32" align="MIDDLE" border="0" src="img186.png" alt="$M$" /> is the size of the vocabulary, whereas complete-link clustering is 
   <!-- MATH
 $\Theta( M_{ave}
N^2\log N )$
 --> <img width="126" height="36" align="MIDDLE" border="0" src="img1658.png" alt="$\Theta( M_{ave}
N^2\log N )$" /> where <img width="38" height="32" align="MIDDLE" border="0" src="img203.png" alt="$ M_{ave}$" /> is the average size of the vocabulary of a document. So for large vocabularies complete-link clustering can be more efficient than an unoptimized implementation of GAAC. We discussed this problem in the context of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means clustering in Chapter <a href="flat-clustering-1.html#ch:flatclust">16</a> (page <a href="k-means-1.html#p:kmedoid">16.4</a> ) and suggested two solutions: truncating centroids (keeping only highly weighted terms) and representing clusters by means of sparse medoids instead of dense centroids. These optimizations can also be applied to GAAC and centroid clustering. </p> 
  <p> Even with these optimizations, HAC algorithms are all <img width="51" height="36" align="MIDDLE" border="0" src="img1579.png" alt="$\Theta(N^2)$" /> or 
   <!-- MATH
 $\Theta(N^2\log N)$
 --> <img width="93" height="36" align="MIDDLE" border="0" src="img1573.png" alt="$
\Theta(N^2 \log N)$" /> and therefore infeasible for large sets of 1,000,000 or more documents. For such large sets, HAC can only be used in combination with a flat clustering algorithm like <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means. Recall that <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means requires a set of seeds as initialization (Figure <a href="k-means-1.html#fig:clusttb2">16.5</a> , page <a href="k-means-1.html#p:clusttb2">16.5</a> ). If these seeds are badly chosen, then the resulting clustering will be of poor quality. We can employ an HAC algorithm to compute seeds of high quality. If the HAC algorithm is applied to a document subset of size <img width="32" height="38" align="MIDDLE" border="0" src="img497.png" alt="$\sqrt{N}$" />, then the overall runtime of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means cum HAC seed generation is <img width="44" height="33" align="MIDDLE" border="0" src="img61.png" alt="$\Theta(N)$" />. This is because the application of a quadratic algorithm to a sample of size <img width="32" height="38" align="MIDDLE" border="0" src="img497.png" alt="$\sqrt{N}$" /> has an overall complexity of <img width="44" height="33" align="MIDDLE" border="0" src="img61.png" alt="$\Theta(N)$" />. An appropriate adjustment can be made for an 
   <!-- MATH
 $\Theta(N^2 \log N)$
 --> <img width="93" height="36" align="MIDDLE" border="0" src="img1573.png" alt="$
\Theta(N^2 \log N)$" /> algorithm to guarantee linearity. This algorithm is referred to as the <a name="p:buckshot"></a> <a name="27183"></a> <i>Buckshot algorithm</i> . It combines the determinism and higher reliability of HAC with the efficiency of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means. </p> 
  <p> </p> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html4449" href="references-and-further-reading-17.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4443" href="hierarchical-clustering-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4437" href="cluster-labeling-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4445" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4447" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4450" href="references-and-further-reading-17.html">References and further reading</a> 
  <b> Up:</b> 
  <a name="tex2html4444" href="hierarchical-clustering-1.html">Hierarchical clustering</a> 
  <b> Previous:</b> 
  <a name="tex2html4438" href="cluster-labeling-1.html">Cluster labeling</a> &nbsp; 
  <b> <a name="tex2html4446" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4448" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>  
 </body>
</html>