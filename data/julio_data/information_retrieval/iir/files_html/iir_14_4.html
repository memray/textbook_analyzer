<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Linear versus nonlinear classifiers</title> 
  <meta name="description" content="Linear versus nonlinear classifiers" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="classification-with-more-than-two-classes-1.html" /> 
  <link rel="previous" href="k-nearest-neighbor-1.html" /> 
  <link rel="up" href="vector-space-classification-1.html" /> 
  <link rel="next" href="classification-with-more-than-two-classes-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html3734" href="classification-with-more-than-two-classes-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3728" href="vector-space-classification-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3722" href="time-complexity-and-optimality-of-knn-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3730" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3732" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3735" href="classification-with-more-than-two-classes-1.html">Classification with more than</a> 
  <b> Up:</b> 
  <a name="tex2html3729" href="vector-space-classification-1.html">Vector space classification</a> 
  <b> Previous:</b> 
  <a name="tex2html3723" href="time-complexity-and-optimality-of-knn-1.html">Time complexity and optimality</a> &nbsp; 
  <b> <a name="tex2html3731" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3733" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION001940000000000000000"></a> <a name="sec:linearvclass"></a> <a name="p:linearvclass"></a> <br /> Linear versus nonlinear classifiers </h1> 
  <p> In this section, we show that the two learning methods Naive Bayes and Rocchio are instances of linear classifiers, the perhaps most important group of text classifiers, and contrast them with nonlinear classifiers. To simplify the discussion, we will only consider two-class classifiers in this section and define a <a name="20294"></a> <i>linear classifier</i> as a two-class classifier that decides class membership by comparing a linear combination of the features to a threshold. </p> 
  <p> </p> 
  <div align="CENTER"> 
   <a name="fig:vclassline"></a> 
   <a name="p:vclassline"></a> 
   <a name="20300"></a> 
   <table> 
    <caption align="BOTTOM"> 
     <strong>Figure 14.8:</strong> There are an infinite number of hyperplanes that separate two linearly separable classes. 
    </caption> 
    <tbody> 
     <tr> 
      <td><img width="225" height="202" align="BOTTOM" border="0" src="img1161.png" alt="\includegraphics[width=6cm]{vclassline.eps}" /></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <p> In two dimensions, a linear classifier is a line. Five examples are shown in Figure <a href="#fig:vclassline">14.8</a> . These lines have the functional form 
   <!-- MATH
 $w_1x_1+w_2x_2=b$
 --> <img width="122" height="31" align="MIDDLE" border="0" src="img1101.png" alt="$w_1x_1+w_2x_2=b$" />. The classification rule of a linear classifier is to assign a document to <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" /> if 
   <!-- MATH
 $w_1x_1+w_2x_2>b$
 --> <img width="121" height="31" align="MIDDLE" border="0" src="img1162.png" alt="$w_1x_1+w_2x_2&gt;b$" /> and to <img width="11" height="32" align="MIDDLE" border="0" src="img931.png" alt="$\overline{c}$" /> if 
   <!-- MATH
 $w_1x_1+w_2x_2\leq b$
 --> <img width="122" height="31" align="MIDDLE" border="0" src="img1163.png" alt="$w_1x_1+w_2x_2\leq b$" />. Here, 
   <!-- MATH
 $(x_1, x_2)^{T}$
 --> <img width="64" height="38" align="MIDDLE" border="0" src="img1164.png" alt="$(x_1, x_2)^{T}$" /> is the two-dimensional vector representation of the document and 
   <!-- MATH
 $(w_1, w_2)^{T}$
 --> <img width="71" height="38" align="MIDDLE" border="0" src="img1165.png" alt="$(w_1, w_2)^{T}$" /> is the parameter vector that defines (together with <img width="12" height="31" align="MIDDLE" border="0" src="img137.png" alt="$b$" />) the decision boundary. An alternative geometric interpretation of a linear classifier is provided in Figure&nbsp;<a href="a-simple-example-of-machine-learned-scoring-1.html#fig:mlr1">15.7</a> (page&nbsp;<a href="a-simple-example-of-machine-learned-scoring-1.html#p:mlr1"><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/crossref.png" /></a>). </p> 
  <p> We can generalize this 2D linear classifier to higher dimensions by defining a hyperplane as we did in Equation&nbsp;<a href="rocchio-classification-1.html#linearclassifier">140</a>, repeated here as Equation&nbsp;<a href="#linearclassifier2">144</a>: <br /> </p> 
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\vec{w}^{T}\vec{x} = b
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody> 
     <tr valign="MIDDLE"> 
      <td align="CENTER" nowrap=""><a name="linearclassifier2"></a><img width="59" height="24" border="0" src="img1166.png" alt="\begin{displaymath}
\vec{w}^{T}\vec{x} = b
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (144)</td> 
     </tr> 
    </tbody> 
   </table> 
   <br clear="ALL" /> 
  </div> 
  <p></p> The assignment criterion then is: assign to 
  <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" /> if 
  <!-- MATH
 $\vec{w}^{T}\vec{x} > b$
 --> 
  <img width="63" height="38" align="MIDDLE" border="0" src="img1167.png" alt="$\vec{w}^{T}\vec{x} &gt; b$" /> and to 
  <img width="11" height="32" align="MIDDLE" border="0" src="img931.png" alt="$\overline{c}$" /> if 
  <!-- MATH
 $\vec{w}^{T}\vec{x} \leq b$
 --> 
  <img width="63" height="38" align="MIDDLE" border="0" src="img1168.png" alt="$\vec{w}^{T}\vec{x} \leq b$" />. We call a hyperplane that we use as a linear classifier a 
  <a name="p:decisionhyperplane"></a> 
  <a name="20326"></a> 
  <i>decision hyperplane</i> . 
  <p> </p> 
  <div align="CENTER"> 
   <a name="fig:linearalgorithm"></a> 
   <a name="p:linearalgorithm"></a> 
   <a name="20340"></a> 
   <table> 
    <caption align="BOTTOM"> 
     <strong>Figure 14.9:</strong> Linear classification algorithm. 
    </caption> 
    <tbody> 
     <tr> 
      <td><img width="237" height="89" border="0" src="img1169.png" alt="\begin{figure}\begin{algorithm}{ApplyLinearClassifier}{\vec{w},b,\vec{x}}
score ...
...in{IF}{score&gt;b}
\RETURN{1}
\ELSE
\RETURN{0}
\end{IF}\end{algorithm}
\end{figure}" /></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <p> The corresponding algorithm for linear classification in <img width="20" height="32" align="MIDDLE" border="0" src="img186.png" alt="$M$" /> dimensions is shown in Figure <a href="#fig:linearalgorithm">14.9</a> . Linear classification at first seems trivial given the simplicity of this algorithm. However, the difficulty is in training the linear classifier, that is, in determining the parameters <img width="16" height="32" align="MIDDLE" border="0" src="img1099.png" alt="$\vec{w}$" /> and <img width="12" height="31" align="MIDDLE" border="0" src="img137.png" alt="$b$" /> based on the training set. In general, some learning methods compute much better parameters than others where our criterion for evaluating the quality of a learning method is the effectiveness of the learned linear classifier on new data. </p> 
  <p> We now show that Rocchio and Naive Bayes are linear classifiers. To see this for Rocchio, observe that a vector <img width="13" height="32" align="MIDDLE" border="0" src="img701.png" alt="$\vec{x}$" /> is on the decision boundary if it has equal distance to the two class centroids: <br /> </p> 
  <div align="CENTER"> 
   <a name="rocchiolinear"></a> 
   <!-- MATH
 \begin{eqnarray}
| \vec{\mu}(c_1) - \vec{x}| = |\vec{\mu}(c_2) - \vec{x} |
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="176" height="33" align="MIDDLE" border="0" src="img1170.png" alt="$\displaystyle \vert \vec{\mu}(c_1) - \vec{x}\vert = \vert\vec{\mu}(c_2) - \vec{x} \vert$" /></td> 
      <td>&nbsp;</td> 
      <td>&nbsp;</td> 
      <td width="10" align="RIGHT"> (145)</td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <br clear="ALL" /> 
  <p></p> Some basic arithmetic shows that this corresponds to a linear classifier with normal vector 
  <!-- MATH
 $\vec{w}=
\vec{\mu}(c_1)-\vec{\mu}(c_2)$
 --> 
  <img width="132" height="33" align="MIDDLE" border="0" src="img1171.png" alt="$\vec{w}=
\vec{\mu}(c_1)-\vec{\mu}(c_2)$" /> and 
  <!-- MATH
 $b=0.5*(|\vec{\mu}(c_1)
|^2-|\vec{\mu}(c_2) |^2)$
 --> 
  <img width="209" height="36" align="MIDDLE" border="0" src="img1172.png" alt="$b=0.5*(\vert\vec{\mu}(c_1)
\vert^2-\vert\vec{\mu}(c_2) \vert^2)$" /> (Exercise 
  <a href="exercises-2.html#ex:exrocchiolinear">14.8</a> ). 
  <p> We can derive the linearity of Naive Bayes from its decision rule, which chooses the category <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" /> with the largest 
   <!-- MATH
 $\hat{P}(c|\onedoc)$
 --> <img width="48" height="38" align="MIDDLE" border="0" src="img1173.png" alt="$\hat{P}(c\vert\onedoc)$" /> (Figure <a href="naive-bayes-text-classification-1.html#fig:multinomialalg">13.2</a> , page <a href="naive-bayes-text-classification-1.html#p:multinomialalg">13.2</a> ) where: <br /> </p> 
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\hat{P}(c|\onedoc) \propto
\hat{P}(c) \prod_{1 \leq k \leq n_d}
\hat{P}(\twasx_k|c)
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody> 
     <tr valign="MIDDLE"> 
      <td align="CENTER" nowrap=""><img width="191" height="46" border="0" src="img1174.png" alt="\begin{displaymath}
\hat{P}(c\vert\onedoc) \propto
\hat{P}(c) \prod_{1 \leq k \leq n_d}
\hat{P}(\twasx_k\vert c)
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (146)</td> 
     </tr> 
    </tbody> 
   </table> 
   <br clear="ALL" /> 
  </div> 
  <p></p> and 
  <img width="20" height="32" align="MIDDLE" border="0" src="img872.png" alt="$n_d$" /> is the number of tokens in the document that are part of the vocabulary. Denoting the complement category as 
  <img width="11" height="32" align="MIDDLE" border="0" src="img1175.png" alt="$\bar{c}$" />, we obtain for the log odds: 
  <br /> 
  <div align="CENTER"> 
   <a name="bayeslinear"></a> 
   <!-- MATH
 \begin{eqnarray}
\log \frac{\hat{P}(c|\onedoc)}{\hat{P}(\bar{c}|\onedoc)} = 
\log  \frac{\hat{P}(c)}{\hat{P}(\bar{c})} + \sum_{1 \leq k
\leq n_d} \log
\frac{\hat{P}(\twasx_k|c)}{\hat{P}(\twasx_k|\bar{c})}
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="302" height="58" align="MIDDLE" border="0" src="img1176.png" alt="$\displaystyle \log \frac{\hat{P}(c\vert\onedoc)}{\hat{P}(\bar{c}\vert\onedoc)} ...
...k
\leq n_d} \log
\frac{\hat{P}(\twasx_k\vert c)}{\hat{P}(\twasx_k\vert\bar{c})}$" /></td> 
      <td>&nbsp;</td> 
      <td>&nbsp;</td> 
      <td width="10" align="RIGHT"> (147)</td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <br clear="ALL" /> 
  <p></p> 
  <p> We choose class <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" /> if the odds are greater than 1 or, equivalently, if the log odds are greater than 0. It is easy to see that Equation&nbsp;<a href="#bayeslinear">147</a> is an instance of Equation&nbsp;<a href="#linearclassifier2">144</a> for 
   <!-- MATH
 $w_i = 
\log
[\hat{P}(\twasx_i|c)/\hat{P}(\twasx_i|\bar{c})]$
 --> <img width="177" height="38" align="MIDDLE" border="0" src="img1177.png" alt="$w_i =
\log
[\hat{P}(\twasx_i\vert c)/\hat{P}(\twasx_i\vert\bar{c})]$" />, <img width="34" height="32" align="MIDDLE" border="0" src="img1178.png" alt="$x_i =$" /> number of occurrences of <img width="14" height="32" align="MIDDLE" border="0" src="img997.png" alt="$t_i$" /> in <img width="12" height="31" align="MIDDLE" border="0" src="img994.png" alt="$\onedoc$" />, and 
   <!-- MATH
 $b = -\log  [ \hat{P}(c) / \hat{P}(\bar{c})]$
 --> <img width="154" height="38" align="MIDDLE" border="0" src="img1179.png" alt="$b = -\log [ \hat{P}(c) / \hat{P}(\bar{c})] $" />. Here, the index <img width="8" height="31" align="MIDDLE" border="0" src="img8.png" alt="$i$" />, 
   <!-- MATH
 $1 \leq i \leq M$
 --> <img width="77" height="31" align="MIDDLE" border="0" src="img974.png" alt="$1 \leq i \leq M$" />, refers to terms of the vocabulary (not to positions in <img width="12" height="31" align="MIDDLE" border="0" src="img354.png" alt="$d$" /> as <img width="11" height="31" align="MIDDLE" border="0" src="img20.png" alt="$k$" /> does; cf. variantmultinomial) and <img width="13" height="32" align="MIDDLE" border="0" src="img701.png" alt="$\vec{x}$" /> and <img width="16" height="32" align="MIDDLE" border="0" src="img1099.png" alt="$\vec{w}$" /> are <img width="20" height="32" align="MIDDLE" border="0" src="img186.png" alt="$M$" />-dimensional vectors. So in log space, Naive Bayes is a linear classifier. </p> 
  <p> <br /></p> 
  <p></p> 
  <div align="CENTER"> 
   <table cellpadding="3" border="1"> 
    <tbody> 
     <tr> 
      <td align="LEFT"><img width="14" height="32" align="MIDDLE" border="0" src="img1180.png" alt="$\twasx_i$" /></td> 
      <td align="LEFT"><img width="20" height="32" align="MIDDLE" border="0" src="img1181.png" alt="$w_i$" /></td> 
      <td align="LEFT"><img width="22" height="31" align="MIDDLE" border="0" src="img1182.png" alt="$d_{1i}$" /></td> 
      <td align="LEFT"><img width="23" height="31" align="MIDDLE" border="0" src="img1183.png" alt="$d_{2i}$" /></td> 
      <td align="LEFT"><img width="14" height="32" align="MIDDLE" border="0" src="img1180.png" alt="$\twasx_i$" /></td> 
      <td align="LEFT"><img width="20" height="32" align="MIDDLE" border="0" src="img1181.png" alt="$w_i$" /></td> 
      <td align="LEFT"><img width="22" height="31" align="MIDDLE" border="0" src="img1182.png" alt="$d_{1i}$" /></td> 
      <td align="LEFT"><img width="23" height="31" align="MIDDLE" border="0" src="img1183.png" alt="$d_{2i}$" /></td> 
     </tr> 
     <tr> 
      <td align="LEFT">prime</td> 
      <td align="LEFT">0.70</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">1</td> 
      <td align="LEFT">dlrs</td> 
      <td align="LEFT">-0.71</td> 
      <td align="LEFT">1</td> 
      <td align="LEFT">1</td> 
     </tr> 
     <tr> 
      <td align="LEFT">rate</td> 
      <td align="LEFT">0.67</td> 
      <td align="LEFT">1</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">world</td> 
      <td align="LEFT">-0.35</td> 
      <td align="LEFT">1</td> 
      <td align="LEFT">0</td> 
     </tr> 
     <tr> 
      <td align="LEFT">interest</td> 
      <td align="LEFT">0.63</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">sees</td> 
      <td align="LEFT">-0.33</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">0</td> 
     </tr> 
     <tr> 
      <td align="LEFT">rates</td> 
      <td align="LEFT">0.60</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">year</td> 
      <td align="LEFT">-0.25</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">0</td> 
     </tr> 
     <tr> 
      <td align="LEFT">discount</td> 
      <td align="LEFT">0.46</td> 
      <td align="LEFT">1</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">group</td> 
      <td align="LEFT">-0.24</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">0</td> 
     </tr> 
     <tr> 
      <td align="LEFT">bundesbank</td> 
      <td align="LEFT">0.43</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">dlr</td> 
      <td align="LEFT">-0.24</td> 
      <td align="LEFT">0</td> 
      <td align="LEFT">0</td> 
     </tr> 
    </tbody> 
   </table> A linear classifier. The dimensions 
   <img width="14" height="32" align="MIDDLE" border="0" src="img1180.png" alt="$\twasx_i$" /> and parameters 
   <img width="20" height="32" align="MIDDLE" border="0" src="img1181.png" alt="$w_i$" /> of a linear classifier for the class interest (as in interest rate) in Reuters-21578. The threshold is 
   <img width="42" height="31" align="MIDDLE" border="0" src="img777.png" alt="$b = 0$" />. Terms like dlr and world have negative weights because they are indicators for the competing class currency. 
   <a name="tab:linclassexample"></a> 
   <a name="p:linclassexample"></a> 
  </div> 
  <br /> 
  <p> <b>Worked example.</b> Table <a href="#tab:linclassexample">14.4</a> defines a linear classifier for the category interest in Reuters-21578 (see Section <a href="evaluation-of-text-classification-1.html#sec:textcat-eval">13.6</a> , page <a href="evaluation-of-text-classification-1.html#p:reuters21578">13.6</a> ). We assign document <img width="19" height="41" align="MIDDLE" border="0" src="img1104.png" alt="$\vec{d}_1$" /> ``rate discount dlrs world'' to interest since 
   <!-- MATH
 $\vec{w}^{T}\vec{d}_1 = 0.67 \cdot 1 + 0.46 \cdot 1 +
(-0.71) \cdot 1 + (-0.35) \cdot 1 = 0.07 >0= b$
 --> <img width="472" height="41" align="MIDDLE" border="0" src="img1184.png" alt="$\vec{w}^{T}\vec{d}_1 = 0.67 \cdot 1 + 0.46 \cdot 1 +
(-0.71) \cdot 1 + (-0.35) \cdot 1 = 0.07 &gt;0= b$" />. We assign <img width="19" height="41" align="MIDDLE" border="0" src="img1105.png" alt="$\vec{d}_2$" /> ``prime dlrs'' to the complement class (not in interest) since 
   <!-- MATH
 $\vec{w}^{T}\vec{d}_2 =
-0.01 \leq b$
 --> <img width="133" height="41" align="MIDDLE" border="0" src="img1185.png" alt="$\vec{w}^{T}\vec{d}_2 =
-0.01 \leq b$" />. For simplicity, we assume a simple binary vector representation in this example: 1 for occurring terms, 0 for non-occurring terms. <b>End worked example.</b> </p> 
  <p> </p> 
  <div align="CENTER"> 
   <p><a name="fig:typicallinear"></a><a name="p:typicallinear"></a></p> 
   <img width="407" height="373" align="BOTTOM" border="0" src="img1186.png" alt="\includegraphics[width=9cm]{newbiasvar.eps}" /> A linear problem with noise. In this hypothetical web page classification scenario, Chinese-only web pages are solid circles and mixed Chinese-English web pages are squares. The two classes are separated by a linear class boundary (dashed line, short dashes), except for three noise documents (marked with arrows). 
  </div> 
  <p> Figure <a href="#fig:typicallinear">14.10</a> is a graphical example of a <a name="20435"></a><i>linear problem</i>, which we define to mean that the underlying distributions <img width="48" height="33" align="MIDDLE" border="0" src="img1187.png" alt="$P(d\vert c)$" /> and 
   <!-- MATH
 $P(d|\overline{c})$
 --> <img width="48" height="33" align="MIDDLE" border="0" src="img1188.png" alt="$P(d\vert\overline{c})$" /> of the two classes are separated by a line. We call this separating line the <a name="20438"></a> <i>class boundary</i> . It is the ``true'' boundary of the two classes and we distinguish it from the <a name="20440"></a> <i>decision boundary</i> that the learning method computes to approximate the class boundary. </p> 
  <p> As is typical in text classification, there are some <a name="20442"></a> <i>noise documents</i> in Figure <a href="#fig:typicallinear">14.10</a> (marked with arrows) that do not fit well into the overall distribution of the classes. In Section <a href="feature-selection-1.html#sec:feature">13.5</a> (page <a href="feature-selection-1.html#p:noisefeature">13.5</a> ), we defined a noise feature as a misleading feature that, when included in the document representation, on average increases the classification error. Analogously, a noise document is a document that, when included in the training set, misleads the learning method and increases classification error. Intuitively, the underlying distribution partitions the representation space into areas with mostly homogeneous class assignments. A document that does not conform with the dominant class in its area is a noise document. </p> 
  <p> Noise documents are one reason why training a linear classifier is hard. If we pay too much attention to noise documents when choosing the decision hyperplane of the classifier, then it will be inaccurate on new data. More fundamentally, it is usually difficult to determine which documents are noise documents and therefore potentially misleading. </p> 
  <p> If there exists a hyperplane that perfectly separates the two classes, then we call the two classes <a name="20447"></a> <i>linearly separable</i> . In fact, if linear separability holds, then there is an infinite number of linear separators (Exercise <a href="#ex:numberlinsep">14.4</a> ) as illustrated by Figure <a href="#fig:vclassline">14.8</a> , where the number of possible separating hyperplanes is infinite. </p> 
  <p> Figure <a href="#fig:vclassline">14.8</a> illustrates another challenge in training a linear classifier. If we are dealing with a linearly separable problem, then we need a criterion for selecting among all decision hyperplanes that perfectly separate the training data. In general, some of these hyperplanes will do well on new data, some will not. </p> 
  <p> </p> 
  <div align="CENTER"> 
   <a name="fig:typicalnonlinear"></a> 
   <a name="p:typicalnonlinear"></a> 
   <a name="20454"></a> 
   <table> 
    <caption align="BOTTOM"> 
     <strong>Figure 14.11:</strong> A nonlinear problem. 
    </caption> 
    <tbody> 
     <tr> 
      <td><img width="278" height="251" align="BOTTOM" border="0" src="img1189.png" alt="\includegraphics[width=7cm]{nonlinear.eps}" /></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <p> An example of a <a name="20458"></a> <i>nonlinear classifier</i> is kNN. The nonlinearity of kNN is intuitively clear when looking at examples like Figure <a href="rocchio-classification-1.html#fig:knnboundaries">14.6</a> . The decision boundaries of kNN (the double lines in Figure <a href="rocchio-classification-1.html#fig:knnboundaries">14.6</a> ) are locally linear segments, but in general have a complex shape that is not equivalent to a line in 2D or a hyperplane in higher dimensions. </p> 
  <p> Figure <a href="#fig:typicalnonlinear">14.11</a> is another example of a <a name="20463"></a>nonlinear problem: there is no good linear separator between the distributions <img width="48" height="33" align="MIDDLE" border="0" src="img1187.png" alt="$P(d\vert c)$" /> and 
   <!-- MATH
 $P(d|\overline{c})$
 --> <img width="48" height="33" align="MIDDLE" border="0" src="img1188.png" alt="$P(d\vert\overline{c})$" /> because of the circular ``enclave'' in the upper left part of the graph. Linear classifiers misclassify the enclave, whereas a nonlinear classifier like kNN will be highly accurate for this type of problem if the training set is large enough. </p> 
  <p> If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier. </p> 
  <p> <b>Exercises.</b> </p> 
  <ul> 
   <li><a name="ex:numberlinsep"></a> <a name="p:numberlinsep"></a> Prove that the number of linear separators of two classes is either infinite or zero. <p> </p></li> 
  </ul> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html3734" href="classification-with-more-than-two-classes-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3728" href="vector-space-classification-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3722" href="time-complexity-and-optimality-of-knn-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3730" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3732" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3735" href="classification-with-more-than-two-classes-1.html">Classification with more than</a> 
  <b> Up:</b> 
  <a name="tex2html3729" href="vector-space-classification-1.html">Vector space classification</a> 
  <b> Previous:</b> 
  <a name="tex2html3723" href="time-complexity-and-optimality-of-knn-1.html">Time complexity and optimality</a> &nbsp; 
  <b> <a name="tex2html3731" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3733" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>  
 </body>
</html>