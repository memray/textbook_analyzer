<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Mutual information</title> 
  <meta name="description" content="Mutual information" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="feature-selectionchi2-feature-selection-1.html" /> 
  <link rel="previous" href="feature-selection-1.html" /> 
  <link rel="up" href="feature-selection-1.html" /> 
  <link rel="next" href="feature-selectionchi2-feature-selection-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html3549" href="feature-selectionchi2-feature-selection-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3543" href="feature-selection-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3537" href="feature-selection-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3545" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3547" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3550" href="feature-selectionchi2-feature-selection-1.html">Feature selectionChi2 Feature selection</a> 
  <b> Up:</b> 
  <a name="tex2html3544" href="feature-selection-1.html">Feature selection</a> 
  <b> Previous:</b> 
  <a name="tex2html3538" href="feature-selection-1.html">Feature selection</a> &nbsp; 
  <b> <a name="tex2html3546" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3548" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h2><a name="SECTION001851000000000000000"></a><a name="16989"></a> <a name="sec:mutualinfo"></a> <a name="p:mutualinfo"></a> <br /> Mutual information </h2> 
  <p> A common feature selection method is to compute <img width="49" height="33" align="MIDDLE" border="0" src="img1004.png" alt="$A(\tcword,c)$" /> as the expected <a name="16992"></a> <i>mutual information</i> (MI) of term <img width="10" height="32" align="MIDDLE" border="0" src="img891.png" alt="$\tcword$" /> and class <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" />.<a name="tex2html136" href="footnode.html#foot17821"><sup><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/footnote.png" /></sup></a> MI measures how much information the presence/absence of a term contributes to making the correct classification decision on <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" />. Formally: <br /> </p>
  <div align="CENTER">
   <a name="mifeatsel"></a> 
   <!-- MATH
 \begin{eqnarray}
I(\wvar;C) &=&
\sum_{e_\tcword \in \{ 1,0 \} }
\sum_{e_c \in \{ 1,0 \} }
P(\wvar=e_\tcword,C=e_c)
\log_2 \frac
{P(\wvar=e_\tcword,C=e_c)}
{P(\wvar=e_\tcword)P(C=e_c)},
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody>
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="56" height="33" align="MIDDLE" border="0" src="img1011.png" alt="$\displaystyle I(\wvar;C)$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="408" height="56" align="MIDDLE" border="0" src="img1012.png" alt="$\displaystyle \sum_{e_\tcword \in \{ 1,0 \} }
\sum_{e_c \in \{ 1,0 \} }
P(\wvar...
...rd,C=e_c)
\log_2 \frac
{P(\wvar=e_\tcword,C=e_c)}
{P(\wvar=e_\tcword)P(C=e_c)},$" /></td> 
      <td width="10" align="RIGHT"> (130)</td>
     </tr> 
    </tbody>
   </table>
  </div> 
  <br clear="ALL" />
  <p></p> where 
  <img width="17" height="32" align="MIDDLE" border="0" src="img957.png" alt="$\wvar$" /> is a random variable that takes values 
  <img width="46" height="32" align="MIDDLE" border="0" src="img1013.png" alt="$e_\tcword=1$" /> (the document contains term 
  <img width="10" height="32" align="MIDDLE" border="0" src="img891.png" alt="$\tcword$" />) and 
  <img width="45" height="32" align="MIDDLE" border="0" src="img1014.png" alt="$e_\tcword=0$" /> (the document does not contain 
  <img width="10" height="32" align="MIDDLE" border="0" src="img891.png" alt="$\tcword$" />), as defined on page 
  <a href="properties-of-naive-bayes-1.html#p:wvar">13.4</a> , and 
  <img width="15" height="32" align="MIDDLE" border="0" src="img616.png" alt="$C$" /> is a random variable that takes values 
  <img width="47" height="32" align="MIDDLE" border="0" src="img1015.png" alt="$e_c=1$" /> (the document is in class 
  <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" />) and 
  <img width="46" height="32" align="MIDDLE" border="0" src="img1016.png" alt="$e_c=0$" /> (the document is not in class 
  <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" />). We write 
  <img width="21" height="32" align="MIDDLE" border="0" src="img1017.png" alt="$\wvar_\tcword$" /> and 
  <img width="20" height="32" align="MIDDLE" border="0" src="img1018.png" alt="$C_c$" /> if it is not clear from context which term 
  <img width="10" height="32" align="MIDDLE" border="0" src="img891.png" alt="$\tcword$" /> and class 
  <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" /> we are referring to. 
  <p> For<a name="17010"></a> MLEs of the probabilities, Equation <a href="#mifeatsel">130</a> is equivalent to Equation&nbsp;<a href="#mifeatsel2">131</a>: <br /> </p>
  <div align="CENTER">
   <a name="mifeatsel2"></a> 
   <!-- MATH
 \begin{eqnarray}
I(\wvar;C) &=&
\frac{\observationo_{11}}{N}\log_2 \frac{N
\observationo_{11}}{\observationo_{1.}\observationo_{.1}}
+\frac{\observationo_{01}}{N}\log_2 \frac{N
\observationo_{01}}{\observationo_{0.}\observationo_{.1}}\\
&&+\, \frac{\observationo_{10}}{N}\log_2 \frac{N
\observationo_{10}}{\observationo_{1.}\observationo_{.0}}
+\frac{\observationo_{00}}{N}\log_2 \frac{N
\observationo_{00}}{\observationo_{0.}\observationo_{.0}}
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody>
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="56" height="33" align="MIDDLE" border="0" src="img1011.png" alt="$\displaystyle I(\wvar;C)$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="248" height="53" align="MIDDLE" border="0" src="img1019.png" alt="$\displaystyle \frac{\observationo_{11}}{N}\log_2 \frac{N
\observationo_{11}}{\o...
...01}}{N}\log_2 \frac{N
\observationo_{01}}{\observationo_{0.}\observationo_{.1}}$" /></td> 
      <td width="10" align="RIGHT"> (131)</td>
     </tr> 
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td>&nbsp;</td> 
      <td align="LEFT" nowrap=""><img width="263" height="53" align="MIDDLE" border="0" src="img1020.png" alt="$\displaystyle +\, \frac{\observationo_{10}}{N}\log_2 \frac{N
\observationo_{10}...
...00}}{N}\log_2 \frac{N
\observationo_{00}}{\observationo_{0.}\observationo_{.0}}$" /></td> 
      <td width="10" align="RIGHT"> (132)</td>
     </tr> 
    </tbody>
   </table>
  </div> 
  <br clear="ALL" />
  <p></p> where the 
  <img width="18" height="32" align="MIDDLE" border="0" src="img1021.png" alt="$\observationo$" />s are counts of documents that have the values of 
  <img width="15" height="32" align="MIDDLE" border="0" src="img1022.png" alt="$e_\tcword$" /> and 
  <img width="17" height="32" align="MIDDLE" border="0" src="img1023.png" alt="$e_c$" /> that are indicated by the two subscripts. For example, 
  <!-- MATH
 $\observationo_{10}$
 --> 
  <img width="29" height="32" align="MIDDLE" border="0" src="img1024.png" alt="$\observationo_{10}$" /> is the number of documents that contain 
  <img width="10" height="32" align="MIDDLE" border="0" src="img891.png" alt="$\tcword$" /> (
  <img width="46" height="32" align="MIDDLE" border="0" src="img1013.png" alt="$e_\tcword=1$" />) and are not in 
  <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" /> (
  <img width="46" height="32" align="MIDDLE" border="0" src="img1016.png" alt="$e_c=0$" />). 
  <!-- MATH
 $\observationo_{1.} = \observationo_{10}+\observationo_{11}$
 --> 
  <img width="119" height="32" align="MIDDLE" border="0" src="img1025.png" alt="$\observationo_{1.} = \observationo_{10}+\observationo_{11}$" /> is the number of documents that contain 
  <img width="10" height="32" align="MIDDLE" border="0" src="img891.png" alt="$\tcword$" /> (
  <img width="46" height="32" align="MIDDLE" border="0" src="img1013.png" alt="$e_\tcword=1$" />) and we count documents independent of class membership (
  <!-- MATH
 $e_c \in \{ 0, 1\}$
 --> 
  <img width="76" height="33" align="MIDDLE" border="0" src="img1026.png" alt="$e_c \in \{ 0, 1\}$" />). 
  <!-- MATH
 $N=\observationo_{00}+\observationo_{01}+\observationo_{10}+\observationo_{11}$
 --> 
  <img width="200" height="32" align="MIDDLE" border="0" src="img1027.png" alt="$N=\observationo_{00}+\observationo_{01}+\observationo_{10}+\observationo_{11}$" /> is the total number of documents. An example of one of the MLE estimates that transform Equation&nbsp;
  <a href="#mifeatsel">130</a> into Equation&nbsp;
  <a href="#mifeatsel2">131</a> is 
  <!-- MATH
 $P(\wvar=1,C=1)=\observationo_{11}/N$
 --> 
  <img width="190" height="33" align="MIDDLE" border="0" src="img1028.png" alt="$P(\wvar=1,C=1)=\observationo_{11}/N$" />. 
  <p> <b>Worked example.</b> <a name="miworkedeg"></a>Consider the class poultry and the term export in Reuters-RCV1. The counts of the number of documents with the four possible combinations of indicator values are as follows: </p>
  <blockquote> 
   <table cellpadding="3" border="1"> 
    <tbody>
     <tr>
      <td align="CENTER" colspan="1">&nbsp;</td> 
      <td align="CENTER" colspan="1">
       <!-- MATH
 $e_c=e_{\class{poultry}}=1$
 --> <img width="124" height="32" align="MIDDLE" border="0" src="img1029.png" alt="$e_c=e_{\class{poultry}}=1$" /></td> 
      <td align="CENTER" colspan="1">
       <!-- MATH
 $e_c = e_{\class{poultry}}=0$
 --> <img width="123" height="32" align="MIDDLE" border="0" src="img1030.png" alt="$e_c = e_{\class{poultry}}=0$" /></td> 
     </tr> 
     <tr>
      <td align="LEFT">
       <!-- MATH
 $e_\tcword=e_{\term{export}} = 1$
 --> <img width="115" height="32" align="MIDDLE" border="0" src="img1031.png" alt="$e_\tcword=e_{\term{export}} = 1$" /></td> 
      <td align="RIGHT">
       <!-- MATH
 $\observationo_{11}=49$
 --> <img width="67" height="32" align="MIDDLE" border="0" src="img1032.png" alt="$ \observationo_{11}=49$" /></td> 
      <td align="RIGHT">
       <!-- MATH
 $\observationo_{10} = 27{,}652$
 --> <img width="95" height="32" align="MIDDLE" border="0" src="img1033.png" alt="$\observationo_{10} = 27{,}652$" /></td> 
     </tr> 
     <tr>
      <td align="LEFT">
       <!-- MATH
 $e_\tcword=e_{\term{export}} = 0$
 --> <img width="114" height="32" align="MIDDLE" border="0" src="img1034.png" alt="$e_\tcword=e_{\term{export}} = 0$" /></td> 
      <td align="RIGHT">
       <!-- MATH
 $\observationo_{01} = 141$
 --> <img width="75" height="32" align="MIDDLE" border="0" src="img1035.png" alt="$ \observationo_{01} = 141$" /></td> 
      <td align="RIGHT">
       <!-- MATH
 $\observationo_{00}=774{,}106$
 --> <img width="103" height="32" align="MIDDLE" border="0" src="img1036.png" alt="$ \observationo_{00}=774{,}106$" /></td> 
     </tr> 
    </tbody>
   </table> 
  </blockquote> After plugging these values into Equation&nbsp;
  <a href="#mifeatsel2">131</a> we get: 
  <p></p> 
  <div align="CENTER"> 
   <!-- MATH
 \begin{eqnarray*}
I(\wvar;C) &= &\frac{49}{801{,}948}
\log_2\frac{
801{,}948 \cdot 49
}
{(49\! + \!27{,}652)
(49\! + \!141)} \\
& & +\,
\frac{141}{801{,}948}
\log_2 \frac{801{,}948 \cdot 141}
{(141\! + \!774{,}106)(49\! + \!141)}\\
& & +\,
\frac{27{,}652}{801{,}948}
\log_2\frac{
801{,}948 \cdot 27{,}652
}
{(49\! + \!27{,}652)
(27{,}652\! + \!774{,}106)} \\
&& +\,
\frac{774{,}106}{801{,}948}
\log_2\frac{
801{,}948 \cdot 774{,}106
}
{(141\! + \!774{,}106)
(27{,}652\! + \!774{,}106)}\\
&\approx& 0.0001105
\end{eqnarray*}
 --> 
   <img width="431" height="191" border="0" src="img1037.png" alt="\begin{eqnarray*}
I(\wvar;C) &amp;= &amp;\frac{49}{801{,}948}
\log_2\frac{
801{,}948 \cd...
... \!774{,}106)
(27{,}652\! + \!774{,}106)}\\
&amp;\approx&amp; 0.0001105
\end{eqnarray*}" />
  </div> 
  <br clear="ALL" />
  <p></p> 
  <p> <b>End worked example.</b> </p>
  <p> To select <img width="11" height="31" align="MIDDLE" border="0" src="img1002.png" alt="$\ktopk$" /> terms 
   <!-- MATH
 $\tcword_1,\ldots,\tcword_\ktopk$
 --> <img width="63" height="32" align="MIDDLE" border="0" src="img1038.png" alt="$\tcword_1,\ldots,\tcword_\ktopk$" /> for a given class, we use the feature selection algorithm in Figure <a href="feature-selection-1.html#fig:featselalg">13.6</a> : We compute the utility measure as 
   <!-- MATH
 $A(\tcword,c)=I(U_\tcword,C_c)$
 --> <img width="133" height="33" align="MIDDLE" border="0" src="img1039.png" alt="$A(\tcword,c)=I(U_\tcword,C_c)$" /> and select the <img width="11" height="31" align="MIDDLE" border="0" src="img1002.png" alt="$\ktopk$" /> terms with the largest values. </p>
  <p> Mutual information measures how much information - in the information-theoretic sense - a term contains about the class. If a term's distribution is the same in the class as it is in the collection as a whole, then 
   <!-- MATH
 $I(\wvar;C) = 0$
 --> <img width="85" height="33" align="MIDDLE" border="0" src="img1040.png" alt="$I(\wvar;C) = 0$" />. MI reaches its maximum value if the term is a perfect indicator for class membership, that is, if the term is present in a document if and only if the document is in the class. </p>
  <p> </p>
  <div align="CENTER">
   <a name="fig:mifeatures"></a>
   <a name="p:mifeatures"></a>
   <a name="17203"></a> 
   <table> 
    <caption align="BOTTOM">
     <strong>Figure 13.7:</strong> Features with high mutual information scores for six Reuters-RCV1 classes.
    </caption> 
    <tbody>
     <tr>
      <td><img width="406" height="422" border="0" src="img1041.png" alt="\begin{figure}\begin{tabular}{lll}
\par
\begin{tabular}{\vert l\vert l\vert}
\mu...
...84\\
\term{team} &amp; 0.0264\\ \hline
\end{tabular}\par
\end{tabular}
\end{figure}" /></td>
     </tr> 
    </tbody>
   </table> 
  </div> 
  <p> Figure <a href="#fig:mifeatures">13.7</a> shows terms with high mutual information scores for the six classes in Figure <a href="the-text-classification-problem-1.html#fig:setupstatclass">13.1</a> .<a name="tex2html138" href="footnode.html#foot17858"><sup><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/footnote.png" /></sup></a> The selected terms (e.g., london, uk, british for the class UK) are of obvious utility for making classification decisions for their respective classes. At the bottom of the list for UK we find terms like peripherals and tonight (not shown in the figure) that are clearly not helpful in deciding whether the document is in the class. As you might expect, keeping the informative terms and eliminating the non-informative ones tends to reduce noise and improve the classifier's accuracy. </p>
  <p> </p>
  <div align="CENTER">
   <a name="fig:mccallum"></a>
   <a name="p:mccallum"></a>
   <a name="17219"></a> 
   <table> 
    <caption align="BOTTOM">
     <strong>Figure 13.8:</strong> Effect of feature set size on accuracy for multinomial and Bernoulli models.
    </caption> 
    <tbody>
     <tr>
      <td><img width="380" height="343" align="BOTTOM" border="0" src="img1042.png" alt="\includegraphics[totalheight=3in]{art/irnbayes7.eps}" /></td>
     </tr> 
    </tbody>
   </table> 
  </div> Such an accuracy increase can be observed in Figure 
  <a href="#fig:mccallum">13.8</a> , which shows 
  <img width="19" height="32" align="MIDDLE" border="0" src="img522.png" alt="$F_1$" /> as a function of vocabulary size after feature selection for Reuters-RCV1.
  <a name="tex2html140" href="footnode.html#foot17224"><sup><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/footnote.png" /></sup></a> Comparing 
  <img width="19" height="32" align="MIDDLE" border="0" src="img522.png" alt="$F_1$" /> at 132,776 features (corresponding to selection of all features) and at 10-100 features, we see that MI feature selection increases 
  <img width="19" height="32" align="MIDDLE" border="0" src="img522.png" alt="$F_1$" /> by about 0.1 for the multinomial model and by more than 0.2 for the Bernoulli model. For the Bernoulli model, 
  <img width="19" height="32" align="MIDDLE" border="0" src="img522.png" alt="$F_1$" /> peaks early, at ten features selected. At that point, the Bernoulli model is better than the multinomial model. When basing a classification decision on only a few features, it is more robust to consider binary occurrence only. For the multinomial model (MI feature selection), the peak occurs later, at 100 features, and its effectiveness recovers somewhat at the end when we use all features. The reason is that the multinomial takes the number of occurrences into account in parameter estimation and classification and therefore better exploits a larger number of features than the Bernoulli model. Regardless of the differences between the two methods, using a carefully selected subset of the features results in better effectiveness than using all features
  <a name="17225"></a>. 
  <p> </p>
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html3549" href="feature-selectionchi2-feature-selection-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3543" href="feature-selection-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3537" href="feature-selection-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3545" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3547" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3550" href="feature-selectionchi2-feature-selection-1.html">Feature selectionChi2 Feature selection</a> 
  <b> Up:</b> 
  <a name="tex2html3544" href="feature-selection-1.html">Feature selection</a> 
  <b> Previous:</b> 
  <a name="tex2html3538" href="feature-selection-1.html">Feature selection</a> &nbsp; 
  <b> <a name="tex2html3546" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3548" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>   
 </body>
</html>