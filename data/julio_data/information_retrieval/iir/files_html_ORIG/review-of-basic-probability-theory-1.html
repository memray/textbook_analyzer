<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Review of basic probability theory</title> 
  <meta name="description" content="Review of basic probability theory" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="the-probability-ranking-principle-1.html" /> 
  <link rel="previous" href="probabilistic-information-retrieval-1.html" /> 
  <link rel="up" href="probabilistic-information-retrieval-1.html" /> 
  <link rel="next" href="the-probability-ranking-principle-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html3012" href="the-probability-ranking-principle-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3006" href="probabilistic-information-retrieval-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3000" href="probabilistic-information-retrieval-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3008" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3010" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3013" href="the-probability-ranking-principle-1.html">The Probability Ranking Principle</a> 
  <b> Up:</b> 
  <a name="tex2html3007" href="probabilistic-information-retrieval-1.html">Probabilistic information retrieval</a> 
  <b> Previous:</b> 
  <a name="tex2html3001" href="probabilistic-information-retrieval-1.html">Probabilistic information retrieval</a> &nbsp; 
  <b> <a name="tex2html3009" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3011" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION001610000000000000000"></a><a name="sec:probirsec"></a> <a name="p:probirsec"></a> <br /> Review of basic probability theory </h1> 
  <p> We hope that the reader has seen a little basic probability theory previously. We will give a very quick review; some references for further reading appear at the end of the chapter. A variable <img width="17" height="32" align="MIDDLE" border="0" src="img167.png" alt="$A$" /> represents an event (a subset of the space of possible outcomes). Equivalently, we can represent the subset via a <a name="13898"></a> <i>random variable</i> <a name="p:rv-defn"></a> , which is a function from outcomes to real numbers; the subset is the domain over which the random variable <img width="17" height="32" align="MIDDLE" border="0" src="img167.png" alt="$A$" /> has a particular value. Often we will not know with certainty whether an event is true in the world. We can ask the probability of the event 
   <!-- MATH
 $0 \le P(A)
\le 1$
 --> <img width="101" height="33" align="MIDDLE" border="0" src="img675.png" alt="$0 \le P(A)
\le 1$" /><a name="Pr-notation"></a>. For two events <img width="17" height="32" align="MIDDLE" border="0" src="img167.png" alt="$A$" /> and <img width="14" height="32" align="MIDDLE" border="0" src="img168.png" alt="$B$" />, the joint event of both events occurring is described by the joint probability <img width="57" height="33" align="MIDDLE" border="0" src="img676.png" alt="$P(A,B)$" />. The conditional probability <img width="56" height="33" align="MIDDLE" border="0" src="img677.png" alt="$P(A\vert B)$" /> expresses the probability of event <img width="17" height="32" align="MIDDLE" border="0" src="img167.png" alt="$A$" /> given that event <img width="14" height="32" align="MIDDLE" border="0" src="img168.png" alt="$B$" /> occurred. The fundamental relationship between joint and conditional probabilities is given by the <a name="13902"></a> <i>chain rule</i> : <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
P(A, B) = P(A \cap B) = P(A|B)P(B) = P(B|A)P(A)
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><a name="chain-rule"></a><img width="359" height="28" border="0" src="img678.png" alt="\begin{displaymath}
P(A, B) = P(A \cap B) = P(A\vert B)P(B) = P(B\vert A)P(A)
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (56)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> Without making any assumptions, the probability of a joint event equals the probability of one of the events multiplied by the probability of the other event conditioned on knowing the first event happened. 
  <p> Writing 
   <!-- MATH
 $P(\overline{A})$
 --> <img width="41" height="37" align="MIDDLE" border="0" src="img679.png" alt="$P(\overline{A})$" /> for the complement of an event, we similarly have: <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
P(\overline{A},B) = P(B| \overline{A})P(\overline{A})
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><img width="163" height="28" border="0" src="img680.png" alt="\begin{displaymath}
P(\overline{A},B) = P(B\vert \overline{A})P(\overline{A})
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (57)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> Probability theory also has a 
  <a name="13913"></a> 
  <i>partition rule</i> , which says that if an event 
  <img width="14" height="32" align="MIDDLE" border="0" src="img168.png" alt="$B$" /> can be divided into an exhaustive set of disjoint subcases, then the probability of 
  <img width="14" height="32" align="MIDDLE" border="0" src="img168.png" alt="$B$" /> is the sum of the probabilities of the subcases. A special case of this rule gives that: 
  <br /> 
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
P(B) = P(A,B) + P(\overline{A}, B)
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><img width="181" height="28" border="0" src="img681.png" alt="\begin{displaymath}
P(B) = P(A,B) + P(\overline{A}, B)
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (58)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> 
  <p> From these we can derive <a name="13918"></a> <i>Bayes' Rule</i> for inverting conditional probabilities: <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \left[\frac{P(B|A)}{\sum_{X \in \{ A, \overline{A}\}} P(B|X)P(X)}\right]P(A)
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><a name="p:bayesrule"></a><a name="eqn:bayesrule"></a><img width="344" height="72" border="0" src="img682.png" alt="\begin{displaymath}P(A\vert B) = \frac{P(B\vert A)P(A)}{P(B)} = \left[\frac{P(...
...{\sum_{X \in \{ A, \overline{A}\}} P(B\vert X)P(X)}\right]P(A)
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (59)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> This equation can also be thought of as a way of updating probabilities. We start off with an initial estimate of how likely the event 
  <img width="17" height="32" align="MIDDLE" border="0" src="img167.png" alt="$A$" /> is when we do not have any other information; this is the 
  <a name="13928"></a> 
  <i>prior probability</i> 
  <img width="41" height="33" align="MIDDLE" border="0" src="img563.png" alt="$P(A)$" />. Bayes' rule lets us derive a 
  <a name="13930"></a> 
  <i>posterior probability</i> 
  <img width="56" height="33" align="MIDDLE" border="0" src="img677.png" alt="$P(A\vert B)$" /> after having seen the evidence 
  <img width="14" height="32" align="MIDDLE" border="0" src="img168.png" alt="$B$" />, based on the 
  <a name="13932"></a> 
  <i>likelihood</i> of 
  <img width="14" height="32" align="MIDDLE" border="0" src="img168.png" alt="$B$" /> occurring in the two cases that 
  <img width="17" height="32" align="MIDDLE" border="0" src="img167.png" alt="$A$" /> does or does not hold.
  <a name="tex2html112" href="footnode.html#foot14463"><sup><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/footnote.png" /></sup></a> 
  <p> Finally, it is often useful to talk about the <a name="13936"></a> <i>odds</i> of an event, which provide a kind of multiplier for how probabilities change: <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\mbox{Odds:\qquad } O(A) = \frac{P(A)}{P(\overline{A})} = \frac{P(A)}{1 - P(A)}
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><a name="O-notation"></a><img width="265" height="46" border="0" src="img683.png" alt="\begin{displaymath}
\mbox{Odds:\qquad } O(A) = \frac{P(A)}{P(\overline{A})} = \frac{P(A)}{1 - P(A)}
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (60)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> 
  <p> </p>
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html3012" href="the-probability-ranking-principle-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3006" href="probabilistic-information-retrieval-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3000" href="probabilistic-information-retrieval-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3008" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3010" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3013" href="the-probability-ranking-principle-1.html">The Probability Ranking Principle</a> 
  <b> Up:</b> 
  <a name="tex2html3007" href="probabilistic-information-retrieval-1.html">Probabilistic information retrieval</a> 
  <b> Previous:</b> 
  <a name="tex2html3001" href="probabilistic-information-retrieval-1.html">Probabilistic information retrieval</a> &nbsp; 
  <b> <a name="tex2html3009" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3011" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>   
 </body>
</html>