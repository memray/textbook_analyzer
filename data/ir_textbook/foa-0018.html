 1.5.2 Computer-Assisted Indexing  The field of library science has studied the manual process of constructing effective indices for a very long time. This standard becomes a useful comparison against which our best automatic techniques can be compared, but it also demonstrates how difficult comparison will be. There are data, for example, that suggest that the capacity of one person (e.g., theindexer) to anticipatethe words used by another person (e.g.,asecond indexer or the query of a subsequent user) is severely limited [Furnas et al, 1987]; we are all quite idiosyncratic in this regard. The lack of interindexer consistency among humans must make us humble in our expectations for automated techniques.  But manual and automatic indexing need not be viewed as competing alternatives. In economic terms, if we had sufficient resources, we could hire enough highly trained catalogers to carefully read every document In a corpus and index each of them. If we couldn't afford this very expensive option, we would have to be satisfied with the best index our automatic system could construct. But if we have enough resources OVERVIEW      29  to hire one or two human indexers, what tools might we give them that would make the most effective use of their time?  We seek methods that leveragethe editorial resource, in the sense that this manual effort does not grow as the corpus does. How might editors and librarians guide an automatic indexing process? What information should this computation provide that would allow intelligent human readers the assurance of a high-quality indexing function? Chapter 7 will discuss ways that editors can train machine learning systems, and a number of analyses that are of interest to editors will be mentioned, especially in Chapter 6.   