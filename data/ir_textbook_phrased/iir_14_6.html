the bias-variance-tradeoff nonlinear classifiers are more powerful than linear-classifiers . for some problems , there exists a nonlinear classifier with zero classification-error , but no such linear classifier . does that mean that we should always use nonlinear classifiers for optimal effectiveness in statistical text classification ? to answer this question , we introduce the bias-variance-tradeoff in this section , one of the most important concepts in machine-learning . the tradeoff helps explain why there is no universally optimal learning-method . selecting an appropriate learning-method is therefore an unavoidable part of solving a text-classification problem . throughout this section , we use linear and nonlinear classifiers as prototypical examples of `` less powerful '' and `` more powerful '' learning , respectively . this is a simplification for a number of reasons . first , many nonlinear models subsume linear-models as a special case . for instance , a nonlinear-learning method like knn will in some cases produce a linear classifier . second , there are nonlinear models that are less complex than linear-models . for instance , a quadratic-polynomial with two parameters is less powerful than a 10,000-dimensional linear classifier . third , the complexity of learning is not really a property of the classifier because there are many aspects of learning (such as feature-selection , cf. feature , regularization , and-constraints such as margin maximization in chapter 15) that make a learning-method either more powerful or less powerful without affecting the type of classifier that is the final result of learning - regardless of whether that classifier is linear or nonlinear . we refer the reader to the publications listed in section 14.7 for a treatment of the bias-variance-tradeoff that takes into account these complexities . in this section , linear and nonlinear classifiers will simply serve as proxies for weaker and stronger learning-methods in text-classification . we first need to state our objective in text-classification more precisely . in section 13.1 (page) , we said that we want to minimize classification-error on the test-set . the implicit assumption was that training-documents and test documents are generated according to the same underlying distribution . we will denote this distribution where is the document and its label or class . graphclassmodelbernoulligraph were examples of generative-models that decompose into the product of and . typicallineartypicalnonlinear depict generative-models for with and . in this section , instead of using the number of correctly classified test documents (or , equivalently , the error-rate on test documents) as evaluation-measure , we adopt an evaluation-measure that addresses the inherent uncertainty of labeling . in many text-classification problems , a given document-representation can arise from documents belonging to different classes . this is because documents from different classes can be mapped to the same document-representation . for example , the one-sentence documents china sues france and france sues china are mapped to the same document-representation in a bag-of-words-model . but only the latter document is relevant to the class legal actions brought by france (which might be defined , for example , as a standing-query by an international-trade lawyer) . to simplify the calculations in this section , we do not count the number of errors on the test-set when evaluating a classifier , but instead look at how well the classifier estimates the conditional-probability of a document being in a class . in the above example , we might have . our goal in text-classification then is to find a classifier such that , averaged over documents , is as close as possible to the true probability . we measure this using mean squared error : (148) we define a classifier to be optimal for a distribution if it minimizes . minimizing mse is a desideratum for classifiers . we also need a criterion for learning-methods . recall that we defined a learning-method as a function that takes a labeled training set as input and returns a classifier . for learning-methods , we adopt as our goal to find a that , averaged over training-sets , learns classifiers with minimal mse . we can formalize this as minimizing learning error : (149) we can use learning error as a criterion for selecting a learning-method in statistical text classification . a learning-method is optimal for a distribution if it minimizes the learning error . (150) (151) (152) (153) (154) (155) (156) (157) (158) (159) 157 150 writing for for better readability , we can transform equation 149 as follows : (160) (161) (162) (163) (164) 162 157 14.6 bias is the squared difference between , the true conditional-probability of being in , and , the prediction of the learned classifier , averaged over training-sets . bias is large if the learning-method produces classifiers that are consistently wrong . bias is small if (i) the classifiers are consistently right or (ii) different training-sets cause errors on different documents or (iii) different training-sets cause positive and negative errors on the same documents , but that average out to close to 0 . if one of these three conditions holds , then , the expectation over all training-sets , is close to . linear methods like rocchio and naive-bayes have a high bias for nonlinear problems because they can only model one type of class boundary , a linear hyperplane . if the generative model has a complex nonlinear class boundary , the bias term in equation 162 will be high because a large number of points will be consistently misclassified . for example , the circular enclave in figure 14.11 does not fit a linear model and will be misclassified consistently by linear-classifiers . we can think of bias as resulting from our domain-knowledge (or lack thereof) that we build into the classifier . if we know that the true boundary between the two classes is linear , then a learning-method that produces linear-classifiers is more likely to succeed than a nonlinear method . but if the true class boundary is not linear and we incorrectly bias the classifier to be linear , then classification-accuracy will be low on average . nonlinear methods like knn have low bias . we can see in figure 14.6 that the decision boundaries of knn are variable - depending on the distribution of documents in the training-set , learned decision boundaries can vary greatly . as a result , each document has a chance of being classified correctly for some training-sets . the average prediction is therefore closer to and bias is smaller than for a linear learning-method . variance is the variation of the prediction of learned classifiers : the average squared difference between and its average . variance is large if different training-sets give rise to very different classifiers . it is small if the training-set has a minor effect on the classification decisions makes , be they correct or incorrect . variance measures how inconsistent the decisions are , not whether they are correct or incorrect . linear-learning methods have low variance because most randomly drawn training-sets produce similar decision hyperplanes . the decision lines produced by linear-learning methods in and 14.11 will deviate slightly from the main class boundaries , depending on the training-set , but the class-assignment for the vast majority of documents (with the exception of those close to the main boundary) will not be affected . the circular enclave in figure 14.11 will be consistently misclassified . nonlinear methods like knn have high variance . it is apparent from figure 14.6 that knn can model very complex boundaries between two classes . it is therefore sensitive to noise documents of the sort depicted in figure 14.10 . as a result the variance term in equation 162 is large for knn : test documents are sometimes misclassified - if they happen to be close to a noise document in the training-set - and sometimes correctly classified - if there are no noise documents in the training-set near them . this results in high variation from training-set to training-set . high-variance learning-methods are prone to overfitting the training-data . the goal in classification is to fit the training-data to the extent that we capture true properties of the underlying distribution . in overfitting , the learning-method also learns from noise . overfitting increases mse and frequently is a problem for high-variance learning-methods . we can also think of variance as the model-complexity or , equivalently , memory capacity of the learning-method - how detailed a characterization of the training-set it can remember and then apply to new data . this capacity corresponds to the number of independent parameters available to fit the training-set . each knn neighborhood makes an independent classification decision . the parameter in this case is the estimate from figure 14.7 . thus , knn 's capacity is only limited by the size of the training-set . it can memorize arbitrarily large-training sets . in contrast , the number of parameters of rocchio is fixed - parameters per dimension , one for each centroid - and independent of the size of the training-set . the rocchio classifier (in form of the centroids defining it) can not `` remember '' fine-grained details of the distribution of the documents in the training-set . according to equation 149 , our goal in selecting a learning-method is to minimize learning error . the fundamental insight captured by equation 162 , which we can succinctly state as : learning-error = bias + variance , is that the learning error has two components , bias and variance , which in general can not be minimized simultaneously . when comparing two learning-methods and , in most cases the comparison comes down to one method having higher bias and lower variance and the other lower bias and higher variance . the decision for one-learning method vs. another is then not simply a matter of selecting the one that reliably produces good classifiers across training-sets (small variance) or the one that can learn classification problems with very difficult decision boundaries (small bias) . instead , we have to weigh the respective merits of bias and variance in our application and choose accordingly . this tradeoff is called the bias-variance-tradeoff . figure 14.10 provides an illustration , which is somewhat contrived , but will be useful as an example for the tradeoff . some chinese-text contains english words written in the roman-alphabet like cpu , online , and gps . consider the task of distinguishing chinese-only web-pages from mixed chinese-english web-pages . a search-engine might offer chinese-users without knowledge of english (but who understand loanwords like cpu) the option of filtering out mixed pages . we use two features for this classification-task : number of roman-alphabet characters and number of chinese-characters on the web-page . as stated earlier , the distribution) of the generative model generates most mixed (respectively , chinese) documents above (respectively , below) the short-dashed line , but there are a few noise documents . in figure 14.10 , we see three classifiers : one-feature classifier . shown as a dotted horizontal line . this classifier uses only one feature , the number of roman-alphabet characters . assuming a learning-method that minimizes the number of misclassifications in the training-set , the position of the horizontal decision-boundary is not greatly affected by differences in the training-set (e.g. , noise documents) . so a learning-method producing this type of classifier has low variance . but its bias is high since it will consistently misclassify squares in the lower left corner and `` solid circle '' documents with more than 50 roman characters . linear classifier . shown as a dashed line with long dashes . learning linear-classifiers has less bias since only noise documents and possibly a few documents close to the boundary between the two classes are misclassified . the variance is higher than for the one-feature classifiers , but still small : the dashed line with long dashes deviates only slightly from the true boundary between the two classes , and so will almost all linear decision boundaries learned from training-sets . thus , very few documents (documents close to the class boundary) will be inconsistently classified . `` fit-training-set-perfectly '' classifier . shown as a solid line . here , the learning-method constructs a decision-boundary that perfectly separates the classes in the training-set . this method has the lowest bias because there is no document that is consistently misclassified - the classifiers sometimes even get noise documents in the test-set right . but the variance of this learning-method is high . because noise documents can move the decision-boundary arbitrarily , test documents close to noise documents in the training-set will be misclassified - something that a linear learning-method is unlikely to do . it is perhaps surprising that so many of the best-known text-classification algorithms are linear . some of these methods , in particular linear svms , regularized logistic-regression and regularized linear-regression , are among the most effective known methods . the bias-variance-tradeoff provides insight into their success . typical classes in text-classification are complex and seem unlikely to be modeled well linearly . however , this intuition is misleading for the high-dimensional spaces that we typically encounter in text applications . with increased dimensionality , the likelihood of linear-separability increases rapidly (exercise 14.8) . thus , linear-models in high-dimensional spaces are quite powerful despite their linearity . even more powerful nonlinear-learning methods can model decision boundaries that are more complex than a hyperplane , but they are also more sensitive to noise in the training-data . nonlinear-learning methods sometimes perform better if the training-set is large , but by no means in all cases .