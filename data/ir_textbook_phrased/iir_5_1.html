statistical-properties of terms in information-retrieval as in the last chapter , we use reuters-rcv1 as our model collection (see table 4.2 , page 4.2) . we give some term and postings statistics for the collection in table 5.1 . `` '' indicates the reduction in size from the previous line . `` t % '' is the cumulative reduction from unfiltered . the table shows the number of terms for different levels of preprocessing (column 2) . the number of terms is the main factor in determining the size of the dictionary . the number of nonpositional postings (column 3) is an indicator of the expected size of the nonpositional index of the collection . the expected size of a positional index is related to the number of positions it must encode (column 4) . in general , the statistics in table 5.1 show that preprocessing affects the size of the dictionary and the number of nonpositional postings greatly . stemming and case folding reduce the number of (distinct) terms by 17 % each and the number of nonpositional postings by 4 % and 3 % , respectively . the treatment of the most frequent words is also important . the rule of 30 states that the 30 most common words account for 30 % of the tokens in written text (31 % in the table) . eliminating the 150 most common words from indexing (as stop words ; cf. section 2.2.2 , page 2.2.2) cuts 25 % to 30 % of the nonpositional postings . but , although a stop list of 150 words reduces the number of postings by a quarter or more , this size reduction does not carry over to the size of the compressed index . as we will see later in this chapter , the postings lists of frequent words require only a few bits per posting after compression . table 5.1 : the effect of preprocessing on the number of terms , nonpositional postings , and tokens for reuters-rcv1 . `` '' indicates the reduction in size from the previous line , except that `` 30 stop words '' and `` 150 stop words '' both use `` case folding '' as their reference line . `` t % '' is the cumulative (`` total '') reduction from unfiltered . we performed stemming with the porter-stemmer (chapter 2 , page 2.2.4) . tokens (number of position (distinct) terms nonpositional postings entries in postings) number t % number t % number t % unfiltered 484,494 109,971,179 197,879,290 no numbers 473,723 2 2 100,680,242 8 8 179,158,204 9 9 case folding 391,523 17 19 96,969,056 3 12 179,158,204 0 9 30 stop words 391,493 0 19 83,390,443 14 24 121,857,825 31 38 150 stop words 391,373 0 19 67,001,847 30 39 94,516,599 47 52 stemming 322,383 17 33 63,812,300 4 42 94,516,599 0 52 the deltas in the table are in a range typical of large-collections . note , however , that the percentage reductions can be very different for some text-collections . for example , for a collection of web-pages with a high proportion of french text , a lemmatizer for french reduces vocabulary-size much more than the porter-stemmer does for an english-only collection because french is a morphologically richer language than english . the compression techniques we describe in the remainder of this chapter are lossless , that is , all information is preserved . better compression ratios can be achieved with lossy-compression , which discards some information . case folding , stemming , and stop-word-elimination are forms of lossy-compression . similarly , the vector-space-model (chapter 6) and dimensionality-reduction techniques like latent-semantic-indexing (chapter 18) create compact representations from which we can not fully restore the original collection . lossy-compression makes sense when the `` lost '' information is unlikely ever to be used by the search-system . for example , web-search is characterized by a large number of documents , short queries , and users who only look at the first few pages of results . as a consequence , we can discard postings of documents that would only be used for hits far down the list . thus , there are retrieval scenarios where lossy methods can be used for compression without any reduction in effectiveness . before introducing techniques for compressing the dictionary , we want to estimate the number of distinct terms in a collection . it is sometimes said that languages have a vocabulary of a certain size . the second edition of the oxford english-dictionary (oed) defines more than 600,000 words . but the vocabulary of most large-collections is much larger than the oed . the oed does not include most names of people , locations , products , or scientific entities like genes . these names need to be included in the inverted-index , so our users can search for them . subsections heaps ' law : estimating the number of terms zipf 's law : modeling the distribution of terms