references and further reading the somewhat quirky name support-vector-machine originates in the neural-networks literature , where learning-algorithms were thought of as architectures , and often referred to as `` machines '' . the distinctive element of this model is that the decision-boundary to use is completely decided (`` supported '') by a few training-data points , the support-vectors . for a more detailed presentation of svms , a good , well-known article-length introduction is (burges , 1998) . chen et al. (2005) introduce the more recent - svm , which provides an alternative parameterization for dealing with inseparable problems , whereby rather than specifying a penalty , you specify a parameter which bounds the number of examples which can appear on the wrong side of the decision surface . there are now also several books dedicated to svms , large-margin-learning , and kernels : (cristianini and shawe-taylor , 2000) and (sch√∂lkopf and smola , 2001) are more mathematically oriented , while (shawe-taylor and cristianini , 2004) aims to be more practical . for the foundations by their originator , see (vapnik , 1998) . some recent , more general books on statistical-learning , such as (hastie et al. , 2001) also give thorough coverage of svms . the construction of multiclass-svms is discussed in (weston and watkins , 1999) , (crammer and singer , 2001) , and (tsochantaridis et al. , 2005) . the last reference provides an introduction to the general-framework of structural svms . the kernel-trick was first presented in (aizerman et al. , 1964) . for more about string kernels and other kernels for structured-data , see (lodhi et al. , 2002) and (gaertner et al. , 2002) . the advances in neural information-processing (nips) conferences have become the premier venue for theoretical machine-learning work , such as on svms . other venues such as sigir are much stronger on experimental-methodology and using text-specific features to improve classifier effectiveness . a recent comparison of most current machine-learning-classifiers (though on problems rather different from typical text problems) can be found in (caruana and niculescu-mizil , 2006) . (li and yang , 2003) , discussed in section 13.6 , is the most recent comparative-evaluation of machine-learning-classifiers on text-classification . older examinations of classifiers on text problems can be found in (yang and liu , 1999 , dumais et al. , 1998 , yang , 1999) . joachims (2002a) presents his work on svms applied to text problems in detail . zhang and oles (2001) present an insightful comparison of naive-bayes , regularized logistic-regression and svm classifiers . joachims (1999) discusses methods of making svm-learning practical over large text data-sets . joachims (2006a) improves on this work . a number of approaches to hierarchical-classification have been developed in order to deal with the common situation where the classes to be assigned have a natural hierarchical organization (weigend et al. , 1999 , dumais and chen , 2000 , koller and sahami , 1997 , mccallum et al. , 1998) . in a recent large study on scaling svms to the entire yahoo! directory , liu et al. (2005) conclude that hierarchical-classification noticeably if still modestly outperforms flat-classification . classifier effectiveness remains limited by the very small number of training-documents for many classes . for a more general approach that can be applied to modeling relations between classes , which may be arbitrary rather than simply the case of a hierarchy , see tsochantaridis et al. (2005) . moschitti and basili (2004) investigate the use of complex nominals , proper nouns and word-senses as features in text-classification . dietterich (2002) overviews ensemble-methods for classifier-combination , while schapire (2003) focuses particularly on boosting , which is applied to text-classification in (schapire and singer , 2000) . chapelle et al. (2006) present an introduction to work in semi-supervised methods , including in particular chapters on using em for semi-supervised text-classification (nigam et al. , 2006) and on transductive svms (joachims , 2006b) . sindhwani and keerthi (2006) present a more efficient-implementation of a transductive-svm for large-data-sets . tong and koller (2001) explore active-learning with svms for text-classification ; baldridge and osborne (2004) point out that examples selected for annotation with one classifier in an active-learning context may be no better than random examples when used with another classifier . machine-learning-approaches to ranking for ad-hoc-retrieval were pioneered in (wong et al. , 1988) , (fuhr , 1992) , and (gey , 1994) . but limited training-data and poor machine-learning-techniques meant that these pieces of work achieved only middling results , and hence they only had limited impact at the time . taylor et al. (2006) study using machine-learning to tune the parameters of the bm25 family of ranking-functions okapi-bm25 so as to maximize ndcg (section 8.4 , page 8.4) . machine-learning-approaches to ordinal-regression appear in (herbrich et al. , 2000) and (burges et al. , 2005) , and are applied to clickstream-data in (joachims , 2002b) . cao et al. (2006) study how to make this approach effective in ir , and qin et al. (2007) suggest an extension involving using multiple hyperplanes . yue et al. (2007) study how to do ranking with a structural-svm approach , and in particular show how this construction can be effectively used to directly optimize for map ranked-evaluation , rather than using surrogate measures like accuracy or area under the roc-curve . geng et al. (2007) study feature-selection for the ranking problem . other approaches to learning-to-rank have also been shown to be effective for web-search , such as (richardson et al. , 2006 , burges et al. , 2005) .