linear-algebra review the rank of a matrix is the number of linearly independent rows (or columns) in it ; thus , . a square matrix all of whose off-diagonal entries are zero is called a diagonal matrix ; its rank is equal to the number of non-zero diagonal entries . if all diagonal entries of such a diagonal matrix are , it is called the identity matrix of dimension and represented by . for a square matrix and a vector that is not all zeros , the values of satisfying (213) eigenvalues 213 right eigenvector principal-eigenvector . left eigenvectors (214) the eigenvalues of a matrix are found by solving the characteristic equation , which is obtained by rewriting equation 213 in the form . the eigenvalues of are then the solutions of , where denotes the determinant of a square matrix . the equation is an th order polynomial-equation in and can have at most roots , which are the eigenvalues of . these eigenvalues can in general be complex , even if all entries of are real . we now examine some further properties of eigenvalues-and-eigenvectors , to set up the central idea of singular value decompositions in section 18.2 below . first , we look at the relationship between matrix-vector-multiplication and eigenvalues . worked example . consider the-matrix (215) (216) (217) (218) (219) (220) (221) end worked example . example 18.1 shows that even though is an arbitrary vector , the effect of multiplication by is determined by the eigenvalues-and-eigenvectors of . furthermore , it is intuitively apparent from equation 221 that the product is relatively unaffected by terms arising from the small eigenvalues of ; in our example , since , the contribution of the third term on the right hand side of equation 221 is small . in fact , if we were to completely ignore the contribution in equation 221 from the third eigenvector corresponding to , then the product would be computed to be rather than the correct product which is ; these two vectors are relatively close to each other by any of various metrics one could apply (such as the length of their vector difference) . this suggests that the effect of small eigenvalues (and their eigenvectors) on a matrix-vector-product is small . we will carry forward this intuition when studying matrix-decompositions and low-rank approximations in section 18.2 . before doing so , we examine the eigenvectors and eigenvalues of special forms of matrices that will be of particular interest to us . for a symmetric-matrix , the eigenvectors corresponding to distinct eigenvalues are orthogonal . further , if is both real and symmetric , the eigenvalues are all real . worked example . consider the real , symmetric-matrix (222) end worked example . subsections matrix-decompositions