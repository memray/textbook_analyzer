features a crawler should provide distributed : the crawler should have the ability to execute in a distributed fashion across multiple machines . scalable : the crawler architecture should permit scaling up the crawl rate by adding extra machines and bandwidth . performance and efficiency : the crawl system should make efficient use of various system-resources including processor , storage and network bandwidth . quality : given that a significant fraction of all web-pages are of poor utility for serving user-query needs , the crawler should be biased towards fetching `` useful '' pages first . freshness : in many applications , the crawler should operate in continuous mode : it should obtain fresh copies of previously fetched pages . a search-engine crawler , for instance , can thus ensure that the search-engine 's index contains a fairly current representation of each indexed web-page . for such continuous crawling , a crawler should be able to crawl a page with a frequency that approximates the rate-of-change of that page . extensible : crawlers should be designed to be extensible in many ways - to cope with new data formats , new fetch protocols , and so on . this demands that the crawler architecture be modular .