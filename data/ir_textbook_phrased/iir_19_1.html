background and history the invention of hypertext , envisioned by vannevar-bush in the 1940 's and first realized in working systems in the 1970 's , significantly precedes the formation of the world-wide-web (which we will simply refer to as the-web) , in the 1990 's . web-usage has shown tremendous growth to the point where it now claims a good fraction of humanity as participants , by relying on a simple , open client-server-design : (1) the server communicates with the client via a protocol (the http or hypertext-transfer-protocol) that is lightweight and simple , asynchronously carrying a variety of payloads (text , images and - over time - richer media such as audio and video files) encoded in a simple markup-language called html (for hypertext markup-language) ; (2) the client - generally a browser , an application within a graphical user environment - can ignore what it does not understand . each of these seemingly innocuous features has contributed enormously to the growth of the-web , so it is worthwhile to examine them further . the basic operation is as follows : a client (such as a browser) sends an http request to a web-server . the browser specifies a url (for uniform-resource-locator) such as http://www.stanford.edu/home/atoz/contact.html . in this example url , the string http refers to the protocol to be used for transmitting the data . the string www.stanford.edu is known as the domain and specifies the root of a hierarchy of web-pages (typically mirroring a filesystem hierarchy underlying the-web server) . in this example , / home/atoz/contact . html is a path in this hierarchy with a file contact.html that contains the information to be returned by the-web server at www.stanford.edu in response to this request . the html-encoded file contact.html holds the hyperlinks and the content (in this instance , contact-information for stanford-university) , as well as formatting rules for rendering this content in a browser . such an http request thus allows us to fetch the content of a page , something that will prove to be useful to us for crawling and indexing documents (chapter 20) . the designers of the first browsers made it easy to view the html markup tags on the content of a url . this simple convenience allowed new-users to create their own html content without extensive training or experience ; rather , they learned from example content that they liked . as they did so , a second feature of browsers supported the rapid proliferation of web-content-creation and usage : browsers ignored what they did not understand . this did not , as one might fear , lead to the creation of numerous incompatible dialects of html . what it did promote was amateur content-creators who could freely experiment with and learn from their newly created web-pages without fear that a simple syntax error would `` bring the system down . '' publishing on the web became a mass activity that was not limited to a few trained programmers , but rather open to tens and eventually hundreds of millions of individuals . for most users and for most information-needs , the-web quickly became the best way to supply and consume information on everything from rare ailments to subway schedules . the mass publishing of information on the web is essentially useless unless this wealth of information can be discovered and consumed by other users . early attempts at making web-information `` discoverable '' fell into two broad categories : (1) full-text index search-engines such as altavista , excite and infoseek and (2) taxonomies populated with web-pages in categories , such as yahoo! the former presented the user with a keyword-search interface supported by inverted-indexes and ranking-mechanisms building on those introduced in earlier chapters . the latter allowed the user to browse through a hierarchical-tree of category labels . while this is at first blush a convenient and intuitive metaphor for finding web-pages , it has a number of drawbacks : first , accurately classifying web-pages into taxonomy tree nodes is for the most part a manual editorial process , which is difficult to scale with the size-of-the-web . arguably , we only need to have `` high-quality '' web-pages in the taxonomy , with only the best web-pages for each category . however , just discovering these and classifying them accurately and consistently into the taxonomy entails significant human effort . furthermore , in order for a user to effectively discover web-pages classified into the nodes of the taxonomy tree , the user 's idea of what sub-tree (s) to seek for a particular topic should match that of the editors performing the classification . this quickly becomes challenging as the size of the taxonomy grows ; the yahoo! taxonomy tree surpassed 1000 distinct nodes fairly early on . given these challenges , the popularity of taxonomies declined over time , even though variants (such as about.com and the open-directory-project) sprang up with subject-matter experts collecting and annotating web-pages for each category . the first generation of web search-engines transported classical search-techniques such as those in the preceding chapters to the-web domain , focusing on the challenge of scale . the earliest web-search-engines had to contend with indexes containing tens of millions of documents , which was a few orders of magnitude larger than any prior-information retrieval-system in the public domain . indexing , query serving and ranking at this scale required the harnessing together of tens of machines to create highly available systems , again at scales not witnessed hitherto in a consumer-facing search application . the first generation of web search-engines was largely successful at solving these challenges while continually indexing a significant fraction of the-web , all the while serving queries with sub-second response-times . however , the quality and relevance of web-search-results left much to be desired owing to the idiosyncrasies of content-creation on the web that we discuss in section 19.2 . this necessitated the invention of new ranking and spam-fighting techniques in order to ensure the quality of the search-results . while classical information retrieval techniques (such as those covered earlier in this book) continue to be necessary for web-search , they are not by any means sufficient . a key aspect (developed further in chapter 21) is that whereas classical techniques measure the relevance of a document to a query , there remains a need to gauge the authoritativeness of a document based on cues such as which website hosts it .