evaluation of unranked retrieval sets given these ingredients , how is system effectiveness measured ? the two most frequent and basic measures for information-retrieval effectiveness are precision-and-recall . these are first defined for the simple case where an ir-system returns a set of documents for a query . we will see later how to extend these notions to ranked-retrieval situations . precision (-rrb- is the fraction of retrieved documents that are relevant (36) recall (-rrb- is the fraction of relevant documents that are retrieved (37) (38) (39) an obvious alternative that may occur to the reader is to judge an information-retrieval-system by its accuracy , that is , the fraction of its classifications that are correct . in terms of the contingency-table above , . this seems plausible , since there are two actual classes , relevant and nonrelevant , and an information-retrieval-system can be thought of as a two-class classifier which attempts to label them as such (it retrieves the subset of documents which it believes to be relevant) . this is precisely the effectiveness measure often used for evaluating machine-learning-classification problems . there is a good reason why accuracy is not an appropriate measure for information-retrieval problems . in almost all circumstances , the data is extremely skewed : normally over 99.9 % of the documents are in the nonrelevant category . a system tuned to maximize accuracy can appear to perform well by simply deeming all documents nonrelevant to all queries . even if the system is quite good , trying to label some documents as relevant will almost always lead to a high rate of false-positives . however , labeling all documents as nonrelevant is completely unsatisfying to an information-retrieval-system user . users are always going to want to see some documents , and can be assumed to have a certain tolerance for seeing some false-positives providing that they get some useful information . the measures of precision-and-recall concentrate the evaluation on the return of true positives , asking what percentage of the relevant documents have been found and how many false-positives have also been returned . the advantage of having the two numbers for precision-and-recall is that one is more important than the other in many circumstances . typical web surfers would like every result on the first page to be relevant (high precision) but have not the slightest interest in knowing let alone looking at every document that is relevant . in contrast , various professional searchers such as paralegals and intelligence-analysts are very concerned with trying to get as high recall as possible , and will tolerate fairly low precision results in order to get it . individuals searching their hard disks are also often interested in high recall searches . nevertheless , the two quantities clearly trade-off against one another : you can always get a recall of 1 (but very low precision) by retrieving all documents for all queries ! recall is a non-decreasing function of the number of documents retrieved . on the other hand , in a good system , precision usually decreases as the number of documents retrieved is increased . in general we want to get some amount of recall while tolerating only a certain percentage of false-positives . a single measure that trades off precision versus recall is the f-measure , which is the weighted harmonic mean of precision-and-recall : (40) balanced f-measure (41) graph comparing the harmonic mean to other means.the graph shows a slice through the calculation of various means of precision-and-recall for the fixed recall value of 70 % . the harmonic mean is always less than either the arithmetic or geometric-mean , and often quite close to the minimum of the two numbers . when the precision is also 70 % , all the measures coincide . why do we use a harmonic mean rather than the simpler average (arithmetic-mean) ? recall that we can always get 100 % recall by just returning all documents , and therefore we can always get a 50 % arithmetic-mean by the same process . this strongly suggests that the arithmetic-mean is an unsuitable measure to use . in contrast , if we assume that 1 document in 10,000 is relevant to the query , the harmonic mean score of this strategy is 0.02 % . the harmonic mean is always less than or equal to the arithmetic-mean and the geometric-mean . when the values of two numbers differ greatly , the harmonic mean is closer to their minimum than to their arithmetic-mean ; see figure 8.1 . exercises . an ir-system returns 8 relevant documents , and 10 nonrelevant documents . there are a total of 20 relevant documents in the collection . what is the precision of the system on this search , and what is its recall ? the balanced f-measure (a.k.a. f) is defined as the harmonic mean of precision-and-recall . what is the advantage of using the harmonic mean rather than `` averaging '' (using the arithmetic-mean) ? derive the equivalence between the two formulas for f-measure shown in equation 40 , given that .