gamma codes table 5.5 : some examples of unary and codes . unary codes are only shown for the smaller numbers . commas in codes are for readability only and are not part of the actual codes . number unary code length offset code 0 0 1 10 0 0 2 110 10 0 10,0 3 1110 10 1 10,1 4 11110 110 00 110,00 9 1111111110 1110 001 1110 ,001 13 1110 101 1110 ,101 24 11110 1000 11110,1000 511 111111110 11111111 111111110,11111111 1025 11111111110 0000000001 11111111110,0000000001 vb codes use an adaptive number of bytes depending on the size of the gap . bit-level codes adapt the length of the code on the finer grained bit level . the simplest bit-level code is unary code . the unary code of is a string of 1s followed by a 0 (see the first two columns of table 5.5) . obviously , this is not a very efficient code , but it will come in handy in a moment . how efficient can a code be in principle ? assuming the gaps with are all equally likely , the optimal encoding uses bits for each . so some gaps (in this case) can not be encoded with fewer than bits . our goal is to get as close to this lower-bound as possible . a method that is within a factor of optimal is encoding . codes implement variable-length encoding by splitting the representation of a gap into a pair of length and offset . offset is in binary , but with the leading 1 removed . for example , for 13 (binary 1101) offset is 101 . length encodes the length of offset in unary code . for 13 , the length of offset is 3 bits , which is 1110 in unary . the code of 13 is therefore 1110101 , the concatenation of length 1110 and offset 101 . the right hand column of table 5.5 gives additional examples of codes . a code is decoded by first reading the unary code up to the 0 that terminates it , for example , the four bits 1110 when decoding 1110101 . now we know how long the offset is : 3 bits . the offset 101 can then be read correctly and the 1 that was chopped off in encoding is prepended : 101 1101 = 13 . the length of offset is bits and the length of length is bits , so the length of the entire code is bits . codes are always of odd length and they are within a factor of 2 of what we claimed to be the optimal encoding length . we derived this optimum from the assumption that the gaps between and are equiprobable . but this need not be the case . in general , we do not know the probability distribution over gaps a priori . figure 5.9 : entropy as a function of for a sample space with two outcomes and . entropy (4) 5.9 it can be shown that the lower-bound for the expected length of a code is if certain conditions hold (see the references) . it can further be shown that for , encoding is within a factor of 3 of this optimal encoding , approaching 2 for large : (5) universal in addition to universality , codes have two other properties that are useful for index-compression . first , they are prefix free , namely , no code is the prefix of another . this means that there is always a unique decoding of a sequence of codes - and we do not need delimiters between them , which would decrease the efficiency of the code . the second property is that codes are parameter free . for many other efficient codes , we have to fit the parameters of a model (e.g. , the binomial-distribution) to the distribution of gaps in the index . this complicates the implementation of compression and decompression . for instance , the parameters need to be stored and retrieved . and in dynamic-indexing , the distribution of gaps can change , so that the original parameters are no longer appropriate . these problems are avoided with a parameter-free code . how much compression of the inverted-index do codes achieve ? to answer this question we use zipf 's law , the term-distribution model introduced in section 5.1.2 . according to zipf 's law , the collection frequency is proportional to the inverse of the rank , that is , there is a constant such that : (6) (7) (8) harmonic number (9) (10) 4.2 figure 5.10 : stratification of terms for estimating the size of a encoded inverted-index . 5.10 encoding the gaps of size with codes , the number of bits needed for the postings list of a term in the th block (corresponding to one row in the figure) is : (11) for reuters-rcv1 , 400,000 and (12) when we run compression on reuters-rcv1 , the actual size of the compressed index is even lower : 101 mb , a bit more than one tenth of the size of the collection . the reason for the discrepancy between predicted and actual value is that (i) zipf 's law is not a very good approximation of the actual distribution of term frequencies for reuters-rcv1 and (ii) gaps are not uniform . the zipf model predicts an index-size of 251 mb for the unrounded numbers from table 4.2 . if term frequencies are generated from the zipf model and a compressed index is created for these artificial terms , then the compressed size is 254 mb . so to the extent that the assumptions about the distribution of term frequencies are accurate , the predictions of the model are correct . table : index and dictionary-compression for reuters-rcv1 . the compression-ratio depends on the proportion of actual text in the collection . reuters-rcv1 contains a large amount of xml-markup . using the two best compression-schemes , encoding and blocking with front-coding , the ratio compressed index to collection-size is therefore especially small for reuters-rcv1 : . . data-structure size in mb dictionary , fixed-width 19.211.2 dictionary , term pointers into string 10.8 7.6 , with blocking , 10.3 7.1 , with blocking front-coding 7.9 5.9 collection (text , xml-markup etc) 3600.0 collection (text) 960.0 term incidence matrix 40,000.0 postings , uncompressed (32-bit words) 400.0 postings , uncompressed (20 bits) 250.0 postings , variable byte encoded 116.0 postings , encoded 101.0 table 5.6 summarizes the compression techniques covered in this chapter . the term incidence matrix (figure 1.1 , page 1.1) for reuters-rcv1 has size bits or 40 gb . the numbers were the collection (3600 mb and 960 mb) are for the encoding of rcv1 of cd , which uses one byte per character , not unicode . codes achieve great compression ratios - about 15 % better than variable byte codes for reuters-rcv1 . but they are expensive to decode . this is because many bit-level operations - shifts and masks - are necessary to decode a sequence of codes as the boundaries between codes will usually be somewhere in the middle of a machine word . as a result , query-processing is more expensive for codes than for variable byte codes . whether we choose variable byte or encoding depends on the characteristics of an application , for example , on the relative weights we give to conserving disk space versus maximizing query response-time . the compression-ratio for the index in table 5.6 is about 25 % : 400 mb (uncompressed , each posting stored as a 32-bit word) versus 101 mb (-rrb- and 116 mb (vb) . this shows that both and vb codes meet the objectives we stated in the beginning of the chapter . index-compression substantially improves time and space efficiency of indexes by reducing the amount of disk space needed , increasing the amount of information that can be kept in the cache , and speeding up data-transfers from disk to memory . exercises . compute variable byte codes for the numbers in tables 5.3 5.5 . compute variable byte and codes for the postings list 777 , 17743 , 294068 , 31251336 . use gaps instead of docids where possible . write binary codes in 8-bit blocks . consider the postings list with a corresponding list of gaps . assume that the length of the postings list is stored separately , so the system knows when a postings list is complete . using variable byte encoding : (i) what is the largest gap you can encode in 1 byte ? (ii) what is the largest gap you can encode in 2 bytes ? (iii) how many bytes will the above postings list require under this encoding ? (count only space for encoding the sequence of numbers .) a little trick is to notice that a gap can not be of length 0 and that the stuff left to encode after shifting can not be 0 . based on these observations : (i) suggest a modification to variable byte encoding that allows you to encode slightly larger gaps in the same amount of space . (ii) what is the largest gap you can encode in 1 byte ? (iii) what is the largest gap you can encode in 2 bytes ? (iv) how many bytes will the postings list in exercise 5.3.2 require under this encoding ? (count only space for encoding the sequence of numbers .) from the following sequence of - coded gaps , reconstruct first the gap sequence and then the postings sequence : 1110001110101011111101101111011 . codes are relatively inefficient for large numbers (e.g. , 1025 in table 5.5) as they encode the length of the offset in inefficient unary code . codes differ from codes in that they encode the first part of the code (length) in code instead of unary code . the encoding of offset is the same . for example , the code of 7 is 10,0,11 (again , we add commas for readability) . 10,0 is the code for length (2 in this case) and the encoding of offset (11) is unchanged . (i) compute the codes for the other numbers in table 5.5 . for what range of numbers is the code shorter than the code ? (ii) code beats variable byte-code in table 5.6 because the index contains stop words and thus many small gaps . show that variable byte-code is more compact if larger gaps dominate . (iii) compare the compression ratios of code and variable byte-code for a distribution of gaps dominated by large gaps . go through the above calculation of index-size and explicitly state all the approximations that were made to arrive at equation 11 . for a collection of your choosing , determine the number of documents and terms and the average length of a document . (i) how large is the inverted-index predicted to be by equation 11 ? (ii) implement an indexer that creates a - compressed inverted-index for the collection . how large is the actual index ? (iii) implement an indexer that uses variable byte encoding . how large is the variable byte encoded index ? table : two gap sequences to be merged in blocked sort-based indexing encoded gap sequence of run 1 1110110111111001011111111110100011111001 encoded gap sequence of run 2 11111010000111111000100011111110010000011111010101 to be able to hold as many postings as possible in main-memory , it is a good idea to compress intermediate index-files during index-construction . (i) this makes merging runs in blocked sort-based indexing more complicated . as an example , work out the - encoded merged sequence of the gaps in table 5.7 . (ii) index-construction is more space-efficient when using compression . would you also expect it to be faster ? (i) show that the size of the vocabulary is finite according to zipf 's law and infinite according to heaps ' law . (ii) can we derive heaps ' law from zipf 's law ?