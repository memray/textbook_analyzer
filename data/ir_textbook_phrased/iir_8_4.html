evaluation of ranked-retrieval results figure 8.2 : precision/recall graph . precision , recall , and the f-measure are set-based measures . they are computed using unordered sets of documents . we need to extend these measures (or to define new measures) if we are to evaluate the ranked-retrieval results that are now standard with search-engines . in a ranked-retrieval context , appropriate sets of retrieved documents are naturally given by the top retrieved documents . for each such set , precision-and-recall values can be plotted to give a precision-recall-curve , such as the one shown in figure 8.2 . precision-recall curves have a distinctive saw-tooth shape : if the document retrieved is nonrelevant then recall is the same as for the top documents , but precision has dropped . if it is relevant , then both precision-and-recall increase , and the curve jags up and to the right . it is often useful to remove these jiggles and the standard way to do this is with an interpolated precision : the interpolated precision at a certain recall level is defined as the highest precision found for any recall level : (42) the justification is that almost anyone would be prepared to look at a few more documents if it would increase the percentage of the viewed set that were relevant (that is , if the precision of the larger set is higher) . interpolated precision is shown by a thinner line in figure 8.2 . with this definition , the interpolated precision at a recall of 0 is well-defined (exercise 8.4) . recall interp . precision 0.0 1.00 0.1 0.67 0.2 0.63 0.3 0.55 0.4 0.45 0.5 0.41 0.6 0.36 0.7 0.29 0.8 0.13 0.9 0.10 1.0 0.08 calculation of 11-point interpolated average precision.this is for the precision-recall-curve shown in figure 8.2 . examining the entire precision-recall-curve is very informative , but there is often a desire to boil this information down to a few numbers , or perhaps even a single number . the traditional way of doing this (used for instance in the first 8 trec ad-hoc evaluations) is the 11-point interpolated average-precision . for each information-need , the interpolated precision is measured at the 11 recall levels of 0.0 , 0.1 , 0.2 , ... , 1.0 . for the precision-recall-curve in figure 8.2 , these 11 values are shown in table 8.1 . for each recall level , we then calculate the arithmetic-mean of the interpolated precision at that recall level for each information-need in the test-collection . a composite precision-recall-curve showing 11 points can then be graphed . figure 8.3 shows an example graph of such results from a representative good system at trec 8 . averaged 11-point precision/recall graph across 50 queries for a representative trec system.the mean-average-precision for this system is 0.2553 . in recent years , other measures have become more common . most standard among the trec community-is mean-average-precision (map) , which provides a single-figure measure of quality across recall levels . among evaluation-measures , map has been shown to have especially good discrimination and stability . for a single information-need , average-precision is the average of the precision value obtained for the set of top documents existing after each relevant-document is retrieved , and this value is then averaged over information-needs . that is , if the set of relevant documents for an information-need is and is the set of ranked-retrieval results from the top result until you get to document , then (43) using map , fixed recall levels are not chosen , and there is no interpolation . the map value for a test-collection is the arithmetic-mean of average-precision values for individual-information needs . (this has the effect of weighting each information-need equally in the final reported number , even if many documents are relevant to some queries whereas very few are relevant to other queries .) calculated map scores normally vary widely across information-needs when measured within a single system , for instance , between 0.1 and 0.7 . indeed , there is normally more agreement in map for an individual-information need across systems than for map scores for different information-needs for the same system . this means that a set of test information-needs must be large and diverse enough to be representative of system effectiveness across different queries . the above measures factor in precision at all recall levels . for many prominent applications , particularly web-search , this may not be germane to users . what matters is rather how many good results there are on the first page or the first three pages . this leads to measuring precision at fixed low levels of retrieved results , such as 10 or 30 documents . this is referred to as `` precision at '' , for example `` precision-at-10 '' . it has the advantage of not requiring any estimate of the size of the set of relevant documents but the disadvantages that it is the least stable of the commonly used evaluation-measures and that it does not average well , since the total number of relevant documents for a query has a strong influence on precision at . an alternative , which alleviates this problem , is r-precision . it requires having a set of known relevant documents , from which we calculate the precision of the top documents returned . (the set may be incomplete , such as when is formed by creating relevance-judgments for the pooled top results of particular systems in a set of experiments .) r-precision adjusts for the size of the set of relevant documents : a perfect system could score 1 on this metric for each query , whereas , even a perfect system could only achieve a precision at 20 of 0.4 if there were only 8 documents in the collection relevant to an information-need . averaging this measure across queries thus makes more sense . this measure is harder to explain to naive users than precision at but easier to explain than map . if there are relevant documents for a query , we examine the top results of a system , and find that are relevant , then by definition , not only is the precision (and hence r-precision) , but the recall of this result-set is also . thus , r-precision turns out to be identical to the break-even point , another measure which is sometimes used , defined in terms of this equality relationship holding . like precision at , r-precision describes only one point on the precision-recall-curve , rather than attempting to summarize effectiveness across the curve , and it is somewhat unclear why you should be interested in the break-even point rather than either the best point on the curve (the point with maximal f-measure) or a retrieval level-of-interest to a particular application (precision at) . nevertheless , r-precision turns out to be highly correlated with map empirically , despite measuring only a single point on the curve . figure 8.4 : the roc-curve corresponding to the precision-recall-curve in figure 8.2 . . another concept sometimes used in evaluation is an roc-curve . (`` roc '' stands for `` receiver-operating-characteristics '' , but knowing that does n't help most people .) an roc-curve plots the true-positive-rate or sensitivity against the false-positive-rate or (-rrb- . here , sensitivity is just another term for recall . the false-positive-rate is given by . figure 8.4 shows the roc-curve corresponding to the precision-recall-curve in figure 8.2 . an roc-curve always goes from the bottom left to the top right of the graph . for a good system , the graph climbs steeply on the left side . for unranked result sets , specificity , given by , was not seen as a very useful notion . because the set of true negatives is always so large , its value would be almost 1 for all information-needs (and , correspondingly , the value of the false-positive-rate would be almost 0) . that is , the `` interesting '' part of figure 8.2 is , a part which is compressed to a small corner of figure 8.4 . but an roc-curve could make sense when looking over the full retrieval spectrum , and it provides another way of looking at the data . in many fields , a common aggregate measure is to report the area under the roc-curve , which is the roc analog of map . precision-recall curves are sometimes loosely referred to as roc curves . this is understandable , but not accurate . a final approach that has seen increasing adoption , especially when employed with machine-learning-approaches to ranking svm-ranking is measures of cumulative-gain , and in particular normalized-discounted-cumulative-gain (ndcg) . ndcg is designed for situations of non-binary notions of relevance (cf. section 8.5.1) . like precision at , it is evaluated over some number of top search-results . for a set of queries , let be the relevance score assessors gave to document for query . then , (44) exercises . what are the possible values for interpolated precision at a recall level of 0 ? must there always be a break-even point between precision-and-recall ? either show there must be or give a counter-example . what is the relationship between the value of and the break-even point ? the dice-coefficient of two sets is a measure of their intersection scaled by their size (giving a value in the range 0 to 1) : (45) show that the balanced f-measure (-rrb- is equal to the dice-coefficient of the retrieved and relevant-document sets . consider an information-need for which there are 4 relevant documents in the collection . contrast two systems run on this collection . their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result) : system 1 r n r n n n n n r r system 2 n r n n r r r n n n what is the map of each system ? which has a higher map ? does this result intuitively make sense ? what does it say about what is important in getting a good map score ? what is the r-precision of each system ? (does it rank the systems the same as map ?) the following list of rs and ns represents relevant (r) and nonrelevant (n) returned documents in a ranked list of 20 documents retrieved in response to a query from a collection of 10,000 documents . the top of the ranked list (the document the system thinks is most likely to be relevant) is on the left of the list . this list shows 6 relevant documents . assume that there are 8 relevant documents in total in the collection . r r n n n n n n r n r n n n r n n n n r what is the precision of the system on the top 20 ? what is the f on the top 20 ? what is the uninterpolated precision of the system at 25 % recall ? what is the interpolated precision at 33 % recall ? assume that these 20 documents are the complete result-set of the system . what is the map for the query ? assume , now , instead , that the system returned the entire 10,000 documents in a ranked list , and these are the first 20 results returned . f . what is the largest possible map that this system could have ? g . what is the smallest possible map that this system could have ? h . in a set of experiments , only the top 20 results are evaluated by hand . the result in (e) is used to approximate the range (f) - (g) . for this example , how large (in absolute terms) can the error for the map be by calculating (e) instead of (f) and (g) for this query ?