nonlinear svms figure 15.6 : projecting data that is not linearly separable into a higher dimensional space can make it linearly separable . with what we have presented so far , data-sets that are linearly separable (perhaps with a few exceptions or some noise) are well-handled . but what are we going to do if the data-set just does n't allow classification by a linear classifier ? let us look at a one-dimensional case . the top data-set in figure 15.6 is straightforwardly classified by a linear classifier but the middle data-set is not . we instead need to be able to pick out an interval . one way to solve this problem is to map the data on to a higher dimensional space and then to use a linear classifier in the higher dimensional space . for example , the bottom part of the figure shows that a linear separator can easily classify the data if we use a quadratic function to map the data into two dimensions (a polar coordinates projection would be another possibility) . the general idea is to map the original feature-space to some higher-dimensional feature-space where the training-set is separable . of course , we would want to do so in ways that preserve relevant dimensions of relatedness between data points , so that the resultant classifier should still generalize well . svms , and also a number of other linear-classifiers , provide an easy and efficient way of doing this mapping to a higher dimensional space , which is referred to as `` the kernel-trick '' . it 's not really a trick : it just exploits the math that we have seen . the svm linear classifier relies on a dot-product between data point vectors . let . then the classifier we have seen so far is : (172) 172 kernel-function worked example . the quadratic-kernel in two dimensions.quad-kernel for 2-dimensional vectors , , consider . we wish to show that this is a kernel , i.e. , that for some . consider . then : (173) (174) (175) (176) end worked example . in the language of functional-analysis , what kinds of functions are valid kernel-functions ? kernel-functions are sometimes more precisely referred to as mercer-kernels , because they must satisfy mercer 's condition : for any such that is finite , we must have that : (177) 15.5 the two commonly used families of kernels are polynomial kernels and radial-basis-functions . polynomial kernels are of the form . the case of is a linear kernel , which is what we had before the start of this section (the constant 1 just changing the threshold) . the case of gives a quadratic-kernel , and is very commonly used . we illustrated the quadratic-kernel in quad-kernel . the most common form of radial-basis-function is a gaussian-distribution , calculated as : (178) 15.5 the world of svms comes with its own language , which is rather different from the language otherwise used in machine-learning . the terminology does have deep roots in mathematics , but it 's important not to be too awed by that terminology . really , we are talking about some quite simple things . a polynomial-kernel allows us to model feature conjunctions (up to the order of the polynomial) . that is , if we want to be able to model occurrences of pairs-of-words , which give distinctive information about topic-classification , not given by the individual words alone , like perhaps operating and system or ethnic and cleansing , then we need to use a quadratic-kernel . if occurrences of triples of words give distinctive information , then we need to use a cubic-kernel . simultaneously you also get the powers of the basic features - for most text applications , that probably is n't useful , but just comes along with the math and hopefully does n't do harm . a radial-basis-function allows you to have features that pick out circles (hyperspheres) - although the decision boundaries become much more complex as multiple such features interact . a string-kernel lets you have features that are character subsequences of terms . all of these are straightforward notions which have also been used in many other places under different names .