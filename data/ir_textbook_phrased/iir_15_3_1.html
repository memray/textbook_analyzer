choosing what kind of classifier to use when confronted with a need to build a text-classifier , the first question to ask is how much training-data is there currently available ? none ? very little ? quite a lot ? or a huge amount , growing every day ? often one of the biggest practical challenges in fielding a machine-learning classifier in real applications is creating or obtaining enough training-data . for many problems and algorithms , hundreds or thousands of examples from each class are required to produce a high-performance classifier and many real-world contexts involve large-sets of categories . we will initially assume that the classifier is needed as soon as possible ; if a lot of time is available for implementation , much of it might be spent on assembling data resources . if you have no labeled-training-data , and especially if there are existing staff knowledgeable about the domain of the data , then you should never forget the solution of using hand-written rules . that is , you write standing queries , as we touched on at the beginning of chapter 13 . for example : if (wheat or grain) and not (whole or bread) then jacobs and rau (1990) hayes and weinstein (1990) 13.4 if you have fairly little data and you are going to train a supervised classifier , then machine-learning theory says you should stick to a classifier with high bias , as we discussed in section 14.6 (page) . for example , there are theoretical and empirical results that naive-bayes does well in such circumstances (forman and cohen , 2004 , ng and jordan , 2001) , although this effect is not necessarily observed in practice with regularized models over textual data (klein and manning , 2002) . at any rate , a very low bias model like a nearest-neighbor model is probably counterindicated . regardless , the quality of the model will be adversely affected by the limited training-data . here , the theoretically interesting answer is to try to apply semi-supervised-training methods . this includes methods such as bootstrapping or the em-algorithm , which we will introduce in section 16.5 (page) . in these methods , the system gets some labeled documents , and a further large supply of unlabeled documents over which it can attempt to learn . one of the big advantages of naive-bayes is that it can be straightforwardly extended to be a semi-supervised-learning algorithm , but for svms , there is also semi-supervised-learning work which goes under the title of transductive svms . see the references for pointers . often , the practical answer is to work out how to get more labeled-data as quickly as you can . the best way to do this is to insert yourself into a process where humans will be willing to label data for you as part of their natural tasks . for example , in many cases humans will sort or route email for their own purposes , and these actions give information about classes . the alternative of getting human labelers expressly for the task of training classifiers is often difficult to organize , and the labeling is often of lower quality , because the labels are not embedded in a realistic task-context . rather than getting people to label all or a random-sample of documents , there has also been considerable research on active-learning , where a system is built which decides which documents a human should label . usually these are the ones on which a classifier is uncertain of the correct classification . this can be effective in reducing annotation costs by a factor of 2-4 , but has the problem that the good documents to label to train one type of classifier often are not the good documents to label to train a different type of classifier . if there is a reasonable amount of labeled-data , then you are in the perfect position to use everything that we have presented about text-classification . for instance , you may wish to use an svm . however , if you are deploying a linear classifier such as an svm , you should probably design an application that overlays a boolean rule-based-classifier over the machine-learning classifier . users frequently like to adjust things that do not come out quite right , and if management gets on the phone and wants the classification of a particular document fixed right now , then this is much easier to do by hand-writing a rule than by working out how to adjust the weights of an svm without destroying the overall classification-accuracy . this is one reason why machine-learning models like decision-trees which produce user-interpretable boolean-like models retain considerable popularity . if a huge amount of data are available , then the choice of classifier probably has little effect on your results and the best choice may be unclear (cf. banko and brill , 2001) . it may be best to choose a classifier based on the scalability of training or even runtime efficiency . to get to this point , you need to have huge amounts of data . the general rule of thumb is that each doubling of the training-data size produces a linear increase in classifier-performance , but with very-large amounts of data , the improvement becomes sub-linear .