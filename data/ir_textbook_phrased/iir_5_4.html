references and further reading heaps ' law was discovered by heaps (1978) . see also baeza-yates and ribeiro-neto (1999) . a detailed study of vocabulary-growth in large-collections is (williams and zobel , 2005) . zipf 's law is due to zipf (1949) . witten and bell (1990) investigate the quality of the fit obtained by the law . other term-distribution models , including k mixture and two-poisson model , are discussed by manning and schütze (1999 , chapter 15) . carmel et al. (2001) , büttcher and clarke (2006) , blanco and barreiro (2007) , and ntoulas and cho (2007) show that lossy-compression can achieve good compression with no or no significant decrease in retrieval-effectiveness . dictionary-compression is covered in detail by witten et al. (1999 , chapter 4) , which is recommended as additional reading . subsection 5.3.1 is based on (scholer et al. , 2002) . the authors find that variable byte codes process queries two times faster than either bit-level compressed indexes or uncompressed indexes with a 30 % penalty in compression-ratio compared with the best bit-level compression-method . they also show that compressed indexes can be superior to uncompressed indexes not only in disk usage , but also in query-processing speed . compared with vb codes , `` variable nibble '' codes showed 5 % to 10 % better compression and up to one third worse effectiveness in one experiment (anh and moffat , 2005) . trotman (2003) also recommends using vb codes unless disk space is at a premium . in recent work , anh and moffat (2006a ; 2005) and zukowski et al. (2006) have constructed word-aligned binary codes that are both faster in decompression and at least as efficient as vb codes . zhang et al. (2007) investigate the increased effectiveness of caching when a number of different compression techniques for postings lists are used on modern hardware . codes (exercise 5.3.2) and codes were introduced by elias (1975) , who proved that both codes are universal . in addition , codes are asymptotically optimal for . codes perform better than codes if large numbers (greater than 15) dominate . a good introduction to information-theory , including the concept of entropy , is (cover and thomas , 1991) . while elias codes are only asymptotically optimal , arithmetic codes (witten et al. , 1999 , section 2.4) can be constructed to be arbitrarily close to the optimum for any . several additional index-compression techniques are covered by witten et al. (1999 ; sections 3.3 and 3.4 and chapter 5) . they recommend using parameterized codes for index-compression , codes that explicitly model the probability distribution of gaps for each term . for example , they show that golomb-codes achieve better compression ratios than codes for large-collections . moffat and zobel (1992) compare several parameterized methods , including llrun (fraenkel and klein , 1985) . the distribution of gaps in a postings list depends on the assignment of docids to documents . a number of researchers have looked into assigning docids in a way that is conducive to the efficient compression of gap sequences (moffat and stuiver , 1996 ; blandford and blelloch , 2002 ; silvestri et al. , 2004 ; blanco and barreiro , 2006 ; silvestri , 2007) . these techniques assign docids in a small range to documents in a cluster where a cluster can consist of all documents in a given time period , on a particular web-site , or sharing another property . as a result , when a sequence of documents from a cluster occurs in a postings list , their gaps are small and can be more effectively compressed . different considerations apply to the compression of term frequencies and word positions than to the compression of docids in postings lists . see scholer et al. (2002) and zobel and moffat (2006) . zobel and moffat (2006) is recommended in general as an in-depth and up-to-date tutorial on inverted-indexes , including index-compression . this chapter only looks at index-compression for boolean-retrieval . for ranked-retrieval (chapter 6) , it is advantageous to order postings according to term-frequency instead of docid . during query-processing , the scanning of many postings lists can then be terminated early because smaller weights do not change the ranking of the highest ranked documents found so far . it is not a good idea to precompute and store weights in the index (as opposed to frequencies) because they can not be compressed as well as integers (see impactordered) . document compression can also be important in an efficient information-retrieval-system . de moura et al. (2000) and brisaboa et al. (2007) describe compression-schemes that allow direct searching of terms and phrases in the compressed text , which is infeasible with standard text-compression utilities like gzip and compress . exercises . we have defined unary codes as being `` 10 '' : sequences of 1s terminated by a 0 . interchanging the roles of 0s and 1s yields an equivalent `` 01 '' unary code . when this 01 unary code is used , the construction of a code can be stated as follows : (1) write down in binary using bits . (2) prepend 0s . (i) encode the numbers in table 5.5 in this alternative code . (ii) show that this method produces a well-defined alternative code in the sense that it has the same length and can be uniquely decoded . unary code is not a universal code in the sense defined above . however , there exists a distribution over gaps for which unary code is optimal . which distribution is this ? give some examples of terms that violate the assumption that gaps all have the same size (which we made when estimating the space requirements of a - encoded index) . what are general characteristics of these terms ? consider a term whose postings list has size , say , . compare the size of the - compressed gap-encoded postings list if the distribution of the term is uniform (i.e. , all gaps have the same size) versus its size when the distribution is not uniform . which compressed postings list is smaller ? work out the sum in equation 12 and show it adds up to about 251 mb . use the numbers in table 4.2 , but do not round , , and the number of vocabulary blocks .