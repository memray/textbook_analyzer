references and further reading general introductions to statistical-classification and machine-learning can be found in (hastie et al. , 2001) , (mitchell , 1997) , and (duda et al. , 2000) , including many important methods (e.g. , decision-trees and boosting) that we do not cover . a comprehensive review of text-classification methods and results is (sebastiani , 2002) . manning and schütze (1999 , chapter 16) give an accessible introduction to text-classification with coverage of decision-trees , perceptrons and maximum-entropy-models . more information on the superlinear time-complexity of learning-methods that are more accurate than naive-bayes can be found in (perkins et al. , 2003) and (joachims , 2006a) . maron and kuhns (1960) described one of the first nb text classifiers . lewis (1998) focuses on the history of nb classification . bernoulli and multinomial models and their accuracy for different collections are discussed by mccallum and nigam (1998) . eyheramendy et al. (2003) present additional nb models . domingos and pazzani (1997) , friedman (1997) , and hand and yu (2001) analyze why nb performs well although its probability-estimates are poor . the first paper also discusses nb 's optimality when the independence assumptions are true of the data . pavlov et al. (2004) propose a modified document-representation that partially addresses the inappropriateness of the independence assumptions . bennett (2000) attributes the tendency of nb probability-estimates to be close to either 0 or 1 to the effect of document-length . ng and jordan (2001) show that nb is sometimes (although rarely) superior to discriminative-methods because it more quickly reaches its optimal error-rate . the basic nb model presented in this chapter can be tuned for better effectiveness (rennie et al. 2003 ; kocz and yih 2007) . the problem of concept-drift and other reasons why state-of-the-art classifiers do not always excel in practice are discussed by forman (2006) and hand (2006) . early uses of mutual-information and for feature-selection in text-classification are lewis and ringuette (1994) and schütze et al. (1995) , respectively . yang and pedersen (1997) review feature-selection methods and their impact on classification effectiveness . they find that pointwise-mutual-information is not competitive with other methods . yang and pedersen refer to expected mutual-information (equation 130) as information-gain (see exercise 13.6 , page 13.6) . (snedecor and cochran , 1989) is a good reference for the test in statistics , including the yates ' correction for continuity for tables . dunning (1993) discusses problems of the test when counts are small . nongreedy feature-selection techniques are described by hastie et al. (2001) . cohen (1995) discusses the pitfalls of using multiple significance-tests and methods to avoid them . forman (2004) evaluates different methods for feature-selection for multiple-classifiers . david d. lewis defines the modapte split at www.daviddlewis.com/resources/testcollections/reuters21578/readme.txtbased on apté et al. (1994) . lewis (1995) describes utility measures for the evaluation of text-classification systems . yang and liu (1999) employ significance-tests in the evaluation of text-classification methods . lewis et al. (2004) find that svms (chapter 15) perform better on reuters-rcv1 than knn and rocchio (chapter 14) .