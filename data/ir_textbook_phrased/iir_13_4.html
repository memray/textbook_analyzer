properties of naive-bayes 11 12 (121) (122) (123) 59 59 122 we can interpret equation 123 as a description of the generative process we assume in bayesian text-classification . to generate a document , we first choose class with probability (top nodes in and 13.5) . the two models differ in the formalization of the second step , the generation of the document given the class , corresponding to the conditional-distribution : (124) (125) it should now be clearer why we introduced the document space in equation 112 when we defined the classification-problem . a critical step in solving a text-classification problem is to choose the document representation . and are two different document representations . in the first case , is the set of all term sequences (or , more precisely , sequences of term tokens) . in the second case , is . we can not use and 125 for text-classification directly . for the bernoulli model , we would have to estimate different parameters , one for each possible combination of values and a class . the number of parameters in the multinomial case has the same order of magnitude.this being a very-large quantity , estimating these parameters reliably is infeasible . to reduce the number of parameters , we make the naive-bayes conditional-independence assumption . we assume that attribute values are independent of each other given the class : (126) (127) figure 13.4 : the multinomial nb model . figure 13.5 : the bernoulli nb model . we illustrate the conditional-independence assumption in and 13.5 . the class china generates values for each of the five term attributes (multinomial) or six binary attributes (bernoulli) with a certain probability , independent of the values of the other attributes . the fact that a document in the class china contains the term taipei does not make it more likely or less likely that it also contains beijing . in reality , the conditional-independence assumption does not hold for text-data . terms are conditionally dependent on each other . but as we will discuss shortly , nb models perform well despite the conditional-independence assumption . even when assuming conditional-independence , we still have too many parameters for the multinomial-model if we assume a different probability-distribution for each position in the document . the position of a term in a document by itself does not carry information about the class . although there is a difference between china sues france and france sues china , the occurrence of china in position 1 versus position 3 of the document is not useful in nb classification because we look at each term separately . the conditional-independence assumption commits us to this way of processing the evidence . also , if we assumed different term distributions for each position , we would have to estimate a different set of parameters for each . the probability of bean appearing as the first term of a coffee document could be different from it appearing as the second term , and so on . this again causes problems in estimation owing to data-sparseness . for these reasons , we make a second independence-assumption for the multinomial-model , positional independence : the conditional-probabilities for a term are the same independent of position in the document . (128) bag-of-words 6 6.2 with conditional and positional independence assumptions , we only need to estimate parameters (multinomial-model) or (bernoulli model) , one for each term-class combination , rather than a number that is at least exponential in , the size of the vocabulary . the independence assumptions reduce the number of parameters to be estimated by several orders of magnitude . to summarize , we generate a document in the multinomial-model (figure 13.4) by first picking a class with where is a random-variable taking values from as values . next we generate term in position with for each of the positions of the document . the all have the same distribution over terms for a given . in the example in figure 13.4 , we show the generation of , corresponding to the one-sentence document beijing and taipei join wto . for a completely specified document-generation model , we would also have to define a distribution over lengths . without it , the multinomial-model is a token generation-model rather than a document-generation model . we generate a document in the bernoulli model (figure 13.5) by first picking a class with and then generating a binary indicator for each term of the vocabulary (-rrb- . in the example in figure 13.5 , we show the generation of , corresponding , again , to the one-sentence document beijing and taipei join wto where we have assumed that and is a stop word . table 13.3 : multinomial versus bernoulli model . multinomial-model bernoulli model event-model generation of token generation of document random-variable (s) iff occurs at given pos iff occurs in doc document-representation parameter-estimation decision-rule : maximize multiple occurrences taken into account ignored length of docs can handle longer docs works best for short docs # features can handle more works best with fewer estimate for term the we compare the two models in table 13.3 , including estimation equations and decision-rules . naive-bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural-language . the conditional-independence assumption states that features are independent of each other given the class . this is hardly ever true for terms in documents . in many cases , the opposite is true . the pairs hong and kong or london and english in figure 13.7 are examples of highly dependent terms . in addition , the multinomial-model makes an assumption of positional independence . the bernoulli model ignores positions in documents altogether because it only cares about absence or presence . this bag-of-words-model discards all information that is communicated by the order of words in natural-language-sentences . how can nb be a good text-classifier when its model of natural-language is so oversimplified ? table 13.4 : correct estimation implies accurate prediction , but accurate prediction does not imply correct estimation . class selected true probability 0.6 0.4 (equation 126) 0.00099 0.00001 nb estimate 0.99 0.01 the answer is that even though the probability estimates of nb are of low quality , its classification decisions are surprisingly good . consider a document with true probabilities and as shown in table 13.4 . assume that contains many terms that are positive indicators for and many terms that are negative indicators for . thus , when using the multinomial-model in equation 126 , will be much larger than (0.00099 vs. 0.00001 in the table) . after division by 0.001 to get well-formed probabilities for , we end up with one estimate that is close to 1.0 and one that is close to 0.0 . this is common : the winning class in nb classification usually has a much larger probability than the other classes and the estimates diverge very significantly from the true probabilities . but the classification decision is based on which class gets the highest score . it does not matter how accurate the estimates are . despite the bad estimates , nb estimates a higher probability for and therefore assigns to the correct class in table 13.4 . correct estimation implies accurate prediction , but accurate prediction does not imply correct estimation . nb classifiers estimate badly , but often classify well . even if it is not the method with the highest accuracy for text , nb has many virtues that make it a strong contender for text-classification . it excels if there are many equally important features that jointly contribute to the classification decision . it is also somewhat robust to noise features (as defined in the next section) and concept-drift - the gradual change over time of the concept underlying a class like us president from bill clinton to george w. bush (see section 13.7) . classifiers like knn knn can be carefully tuned to idiosyncratic properties of a particular time period . this will then hurt them when documents in the following time period have slightly different properties . the bernoulli model is particularly robust with respect to concept-drift . we will see in figure 13.8 that it can have decent performance when using fewer than a dozen terms . the most important indicators for a class are less likely to change . thus , a model that only relies on these features is more likely to maintain a certain level of accuracy in concept-drift . nb 's main strength is its efficiency : training and classification can be accomplished with one pass over the data . because it combines efficiency with good accuracy it is often used as a baseline in text-classification-research . it is often the method of choice if (i) squeezing out a few extra percentage points of accuracy is not worth the trouble in a text-classification application , (ii) a very-large amount of training-data is available and there is more to be gained from training on a lot of data than using a better classifier on a smaller training-set , or (iii) if its robustness to concept-drift can be exploited . table 13.5 : a set of documents for which the nb independence assumptions are problematic . (1) he moved from london , ontario , to london , england . (2) he moved from london , england , to london , ontario . (3) he moved from england to london , ontario . in this book , we discuss nb as a classifier for text . the independence assumptions do not hold for text . however , it can be shown that nb is an optimal classifier (in the sense of minimal error-rate on new data) for data where the independence assumptions do hold . subsections a variant of the multinomial-model