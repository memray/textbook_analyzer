soft margin classification for the very high dimensional problems common in text-classification , sometimes the data are linearly separable . but in the general case they are not , and even if they are , we might prefer a solution that better separates the bulk of the data while ignoring a few weird noise documents . figure 15.5 : large-margin-classification with slack variables . if the training-set is not linearly separable , the standard approach is to allow the fat decision margin to make a few mistakes (some points - outliers or noisy examples - are inside or on the wrong side of the margin) . we then pay a cost for each misclassified example , which depends on how far it is from meeting the margin requirement given in equation 169 . to implement this , we introduce slack variables . a non-zero value for allows to not meet the margin requirement at a cost proportional to the value of . see figure 15.5 . the formulation of the svm optimization-problem with slack variables is : the optimization-problem is then trading-off how fat it can make the margin versus how many points have to be moved around to allow this margin . the margin can be less than 1 for a point by setting , but then one pays a penalty of in the minimization for having done that . the sum of the gives an upper-bound on the number of training errors . soft-margin svms minimize training error traded off against margin . the parameter is a regularization term , which provides a way to control overfitting : as becomes large , it is unattractive to not respect the data at the cost of reducing the geometric margin ; when it is small , it is easy to account for some data points with the use of slack variables and to have a fat margin placed so it models the bulk of the data . the dual problem for soft margin classification becomes : neither the slack variables nor lagrange-multipliers for them appear in the dual problem . all we are left with is the constant bounding the possible size of the lagrange-multipliers for the support-vector data points . as before , the with non-zero will be the support-vectors . the solution of the dual problem is of the form : again is not needed explicitly for classification , which can be done in terms of dot products with data points , as in equation 170 . typically , the support-vectors will be a small proportion of the training-data . however , if the problem is non-separable or with small margin , then every data point which is misclassified or within the margin will have a non-zero . if this set of points becomes large , then , for the nonlinear case which we turn to in section 15.2.3 , this can be a major slowdown for using svms at test-time . classifier mode method time-complexity nb training nb testing rocchio training rocchio testing knn training preprocessing knn testing preprocessing knn training no preprocessing knn testing no preprocessing svm training conventional ; , empirically svm training cutting-planes svm testing training and testing complexity of various classifiers including svms . training is the time the learning-method takes to learn a classifier over , while testing is the time it takes a classifier to classify one document . for svms , multiclass-classification is assumed to be done by a set of one-versus-rest classifiers . is the average number of tokens per document , while is the average vocabulary (number of non-zero features) of a document . and are the numbers of tokens and types , respectively , in the test document . the complexity of training and testing with linear svms is shown in table 15.1 . the time for training an svm is dominated by the time for solving the underlying qp , and so the theoretical and empirical complexity varies depending on the method used to solve it . the standard result for solving qps is that it takes time cubic in the size of the data-set (kozlov et al. , 1979) . all the recent work on svm training has worked to reduce that complexity , often by being satisfied with approximate-solutions . standardly , empirical complexity is about (joachims , 2006a) . nevertheless , the super-linear training-time of traditional svm algorithms makes them difficult or impossible to use on very-large training-data sets . alternative traditional svm solution algorithms which are linear in the number of training-examples scale badly with a large number of features , which is another standard attribute of text problems . however , a new training-algorithm based on cutting-plane techniques gives a promising answer to this issue by having running-time linear in the number of training-examples and the number of non-zero features in examples (joachims , 2006a) . nevertheless , the actual speed of doing quadratic-optimization remains much slower than simply counting terms as is done in a naive-bayes-model . extending svm algorithms to nonlinear svms , as in the next section , standardly increases training complexity by a factor of (since dot products between examples need to be calculated) , making them impractical . in practice it can often be cheaper to materialize the higher-order features and to train a linear svm .