time-complexity and optimality of knn knn with preprocessing of training-set training testing knn without preprocessing of training-set training testing training and test times for knn-classification . is the average size of the vocabulary of documents in the collection . table 14.3 gives the time-complexity of knn . knn has properties that are quite different from most other classification algorithms . training a knn-classifier simply consists of determining and preprocessing documents . in fact , if we preselect a value for and do not preprocess , then knn requires no training at all . in practice , we have to perform preprocessing steps like tokenization . it makes more sense to preprocess training-documents once as part of the training-phase rather than repeatedly every time we classify a new test document . test-time is for knn . it is linear in the size of the training-set as we need to compute the distance of each training-document from the test document . test-time is independent of the number of classes . knn therefore has a potential advantage for problems with large . in knn-classification , we do not perform any estimation of parameters as we do in rocchio classification (centroids) or in naive-bayes (priors and conditional-probabilities) . knn simply memorizes all examples in the training-set and then compares the test document to them . for this reason , knn is also called memory-based-learning or instance-based-learning . it is usually desirable to have as much training-data as possible in machine-learning . but in knn large-training sets come with a severe efficiency penalty in classification . can knn testing be made more efficient than or , ignoring the length of documents , more efficient than ? there are fast knn algorithms for small dimensionality (exercise 14.8) . there are also approximations for large that give error-bounds for specific efficiency gains (see section 14.7) . these approximations have not been extensively tested for text-classification applications , so it is not clear whether they can achieve much better efficiency than without a significant loss of accuracy . the reader may have noticed the similarity between the problem of finding nearest-neighbors of a test document and ad-hoc-retrieval , where we search for the documents with the highest similarity to the query (section 6.3.2 , page 6.3.2) . in fact , the two problems are both nearest-neighbor problems and only differ in the relative density of (the vector of) the test document in knn (10s or 100s of non-zero entries) versus the sparseness of (the vector of) the query in ad-hoc-retrieval (usually fewer than 10 non-zero entries) . we introduced the inverted-index for efficient ad-hoc-retrieval in section 1.1 (page 1.1) . is the inverted-index also the solution for efficient knn ? an inverted-index restricts a search to those documents that have at least one term in common with the query . thus in the context of knn , the inverted-index will be efficient if the test document has no term overlap with a large number of training-documents . whether this is the case depends on the classification-problem . if documents are long and no stop list is used , then less time will be saved . but with short documents and a large stop list , an inverted-index may well cut the average test-time by a factor of 10 or more . the search-time in an inverted-index is a function of the length of the postings lists of the terms in the query . postings lists grow sublinearly with the length of the collection since the vocabulary increases according to heaps ' law - if the probability of occurrence of some terms increases , then the probability of occurrence of others must decrease . however , most new terms are infrequent . we therefore take the complexity of inverted-index search to be (as discussed in section 2.4.2 , page 2.4.2) and , assuming average document-length does not change over time , . as we will see in the next chapter , knn 's effectiveness is close to that of the most accurate learning-methods in text-classification (table 15.2 , page 15.2) . a measure of the quality of a learning-method is its bayes error-rate , the average-error-rate of classifiers learned by it for a particular problem . knn is not optimal for problems with a non-zero bayes error-rate - that is , for problems where even the best possible classifier has a non-zero classification-error . the error of 1nn is asymptotically (as the training-set increases) bounded by twice the bayes error-rate . that is , if the optimal classifier has an error-rate of , then 1nn has an asymptotic error-rate of less than . this is due to the effect of noise - we already saw one example of noise in the form of noisy features in section 13.5 (page 13.5) , but noise can also take other forms as we will discuss in the next section . noise affects two components of knn : the test document and the closest training-document . the two sources of noise are additive , so the overall error of 1nn is twice the optimal error-rate . for problems with bayes error-rate 0 , the error-rate of 1nn will approach 0 as the size of the training-set increases . exercises . explain why knn handles multimodal classes better than rocchio .