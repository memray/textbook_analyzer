result-ranking by machine-learning the above ideas can be readily generalized to functions of many more than two variables . there are lots of other scores that are indicative of the relevance of a document to a query , including static quality (pagerank-style measures , discussed in chapter 21) , document-age , zone contributions , document-length , and so on . providing that these measures can be calculated for a training-document collection with relevance-judgments , any number of such measures can be used to train a machine-learning classifier . for instance , we could train an svm over binary relevance-judgments , and order documents based on their probability-of-relevance , which is monotonic with the documents ' signed distance from the decision-boundary . however , approaching ir result-ranking like this is not necessarily the right way to think about the problem . statisticians normally first divide problems into classification problems (where a categorical variable is predicted) versus regression-problems (where a real number is predicted) . in between is the specialized field of ordinal-regression where a ranking is predicted . machine-learning for ad-hoc-retrieval is most properly thought of as an ordinal-regression-problem , where the goal is to rank a set of documents for a query , given training-data of the same sort . this formulation gives some additional power , since documents can be evaluated relative to other candidate documents for the same query , rather than having to be mapped to a global scale of goodness , while also weakening the problem-space , since just a ranking is required rather than an absolute measure of relevance . issues of ranking are especially germane in web-search , where the ranking at the very top of the results list is exceedingly important , whereas decisions of relevance of a document to a query may be much less important . such work can and has been pursued using the structural-svm framework which we mentioned in section 15.2.2 , where the class being predicted is a ranking of results for a query , but here we will present the slightly simpler ranking-svm . the construction of a ranking-svm proceeds as follows . we begin with a set of judged queries . for each training query , we have a set of documents returned in response to the query , which have been totally ordered by a person for relevance to the query . we construct a vector of features for each document/query pair , using features such as those discussed in section 15.4.1 , and many more . for two documents and , we then form the vector of feature differences : (180) by hypothesis , one of and has been judged more relevant . if is judged more relevant than , denoted (should precede in the results ordering) , then we will assign the vector the class ; otherwise . the goal then is to build a classifier which will return (181) both of the methods that we have just looked at use a linear weighting of document features that are indicators of relevance , as has most work in this area . it is therefore perhaps interesting to note that much of traditional ir weighting involves nonlinear scaling of basic measurements (such as log-weighting of term-frequency , or idf) . at the present time , machine-learning is very good at producing optimal weights for features in a linear combination (or other similar restricted model-classes) , but it is not good at coming up with good nonlinear scalings of basic measurements . this area remains the domain of human feature-engineering . the idea of learning ranking-functions has been around for a number of years , but it is only very recently that sufficient machine-learning knowledge , training-document collections , and computational-power have come together to make this method practical and exciting . it is thus too early to write something definitive on machine-learning-approaches to ranking-in-information-retrieval , but there is every reason to expect the use and importance of machine learned ranking approaches to grow over time . while skilled humans can do a very good job at defining ranking-functions by hand , hand-tuning is difficult , and it has to be done again for each new document-collection and class of users . exercises . plot the first 7 rows of table 15.3 in the - plane to produce a figure like that in figure 15.7 . write down the equation of a line in the - plane separating the rs from the ns . give a training-example (consisting of values for and the relevance-judgment) that when added to the training-set makes it impossible to separate the r 's from the n 's using a line in the - plane .