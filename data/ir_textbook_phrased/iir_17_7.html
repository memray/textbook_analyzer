cluster-labeling in many applications of flat clustering and hierarchical-clustering , particularly in analysis tasks and in user-interfaces (see applications in table 16.1 , page 16.1) , human users interact with clusters . in such settings , we must label clusters , so that users can see what a cluster is about . differential cluster-labeling selects cluster-labels by comparing the distribution of terms in one cluster with that of other clusters . the feature-selection methods we introduced in section 13.5 (page) can all be used for differential cluster-labeling . in particular , mutual-information (mi) (section 13.5.1 , page 13.5.1) or , equivalently , information-gain and the - test (section 13.5.2 , page 13.5.2) will identify cluster-labels that characterize one cluster in contrast to other clusters . a combination of a differential test with a penalty for rare terms often gives the best labeling results because rare terms are not necessarily representative of the cluster as a whole . labeling method # docs centroid mutual-information title 4 622 oil plant mexico production crude power 000 refinery gas bpd plant oil production barrels crude bpd mexico dolly capacity petroleum mexico : hurricane dolly heads for mexico coast 9 1017 police security russian people military peace killed told grozny court police killed military security peace told troops forces rebels people russia : russia 's lebed meets rebel chief in chechnya 10 1259 00 000 tonnes traders futures wheat prices cents september tonne delivery traders futures tonne tonnes desk wheat prices 000 00 usa : export business - grain/oilseeds complex automatically computed cluster labels.this is for three of ten clusters (4 , 9 , and 10) in a - means clustering of the first 10,000 documents in reuters-rcv1 . the last three columns show cluster summaries computed by three labeling methods : most highly weighted terms in centroid (centroid) , mutual-information , and the title of the document closest to the centroid of the cluster (title) . terms selected by only one of the first two methods are in bold . we apply three labeling methods to a - means clustering in table 17.2 . in this example , there is almost no difference between mi and . we therefore omit the latter . cluster-internal labeling computes a label that solely depends on the cluster itself , not on other clusters . labeling a cluster with the title of the document closest to the centroid is one cluster-internal method . titles are easier to read than a list of terms . a full title can also contain important context that did n't make it into the top 10 terms selected by mi . on the web , anchor-text can play a role similar to a title since the anchor-text pointing to a page can serve as a concise summary of its contents . in table 17.2 , the title for cluster 9 suggests that many of its documents are about the chechnya conflict , a fact the mi terms do not reveal . however , a single-document is unlikely to be representative of all documents in a cluster . an example is cluster 4 , whose selected title is misleading . the main topic of the cluster is oil . articles about hurricane dolly only ended up in this cluster because of its effect on oil prices . we can also use a list of terms with high weights in the centroid of the cluster as a label . such highly weighted terms (or , even better , phrases , especially noun-phrases) are often more representative of the cluster than a few titles can be , even if they are not filtered for distinctiveness as in the differential methods . however , a list of phrases takes more time to digest for users than a well crafted title . cluster-internal methods are efficient , but they fail to distinguish terms that are frequent in the collection as a whole from those that are frequent only in the cluster . terms like year or tuesday may be among the most frequent in a cluster , but they are not helpful in understanding the contents of a cluster with a specific topic like oil . in table 17.2 , the centroid-method selects a few more uninformative terms (000 , court , cents , september) than mi (forces , desk) , but most of the terms selected by either method are good descriptors . we get a good sense of the documents in a cluster from scanning the selected terms . for hierarchical-clustering , additional complications arise in cluster-labeling . not only do we need to distinguish an internal node in the tree from its siblings , but also from its parent and its children . documents in child nodes are by definition also members of their parent node , so we can not use a naive differential method to find labels that distinguish the parent from its children . however , more complex criteria , based on a combination of overall collection frequency and prevalence in a given cluster , can determine whether a term is a more informative label for a child node or a parent node (see section 17.9) .