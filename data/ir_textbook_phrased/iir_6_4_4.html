pivoted normalized document-length in section 6.3.1 we normalized each document-vector by the euclidean length of the vector , so that all document-vectors turned into unit vectors . in doing so , we eliminated all information on the length of the original document ; this masks some subtleties about longer documents . first , longer documents will - as a result of containing more terms - have higher tf values . second , longer documents contain more distinct terms . these factors can conspire to raise the scores of longer documents , which (at least for some information-needs) is unnatural . longer documents can broadly be lumped into two categories : (1) verbose documents that essentially repeat the same content - in these , the length of the document does not alter the relative weights of different terms ; (2) documents covering multiple different topics , in which the search-terms probably match small segments of the document but not all of it - in this case , the relative weights of terms are quite different from a single short document that matches the query terms . compensating for this phenomenon is a form of document-length-normalization that is independent of term and document frequencies . to this end , we introduce a form of normalizing the vector-representations of documents in the collection , so that the resulting `` normalized '' documents are not necessarily of unit length . then , when we compute the dot-product score between a (unit) query-vector and such a normalized document , the score is skewed to account for the effect of document-length on relevance . this form of compensation for document-length is known as pivoted document-length-normalization . figure 6.16 : pivoted document-length-normalization . consider a document-collection together with an ensemble of queries for that collection . suppose that we were given , for each query and for each document , a boolean judgment of whether or not is relevant to the query ; in chapter 8 we will see how to procure such a set of relevance-judgments for a query ensemble and a document-collection . given this set of relevance-judgments , we may compute a probability-of-relevance as a function of document-length , averaged over all queries in the ensemble . the resulting plot may look like the curve drawn in thick lines in figure 6.16 . to compute this curve , we bucket documents by length and compute the fraction of relevant documents in each bucket , then plot this fraction against the median document-length of each bucket . (thus even though the `` curve '' in figure 6.16 appears to be continuous , it is in fact a histogram of discrete buckets of document-length .) on the other hand , the curve in thin lines shows what might happen with the same documents and query ensemble if we were to use relevance as prescribed by cosine normalization equation 27 - thus , cosine normalization has a tendency to distort the computed relevance vis-Ã -vis the true relevance , at the expense of longer documents . the thin and thick curves crossover at a point corresponding to document-length , which we refer to as the pivot length ; dashed lines mark this point on the and axes . the idea of pivoted document-length-normalization would then be to `` rotate '' the cosine normalization curve counter-clockwise about so that it more closely matches thick line representing the relevance vs. document-length curve . as mentioned at the beginning of this section , we do so by using in equation 27 a normalization-factor for each document-vector that is not the euclidean length of that vector , but instead one that is larger than the euclidean length for documents of length less than , and smaller for longer documents . to this end , we first note that the normalizing term for in the denominator of equation 27 is its euclidean length , denoted . in the simplest implementation of pivoted document-length-normalization , we use a normalization-factor in the denominator that is linear in , but one of slope as in figure 6.17 . in this figure , the axis represents , while the axis represents possible normalization factors we can use . the thin line depicts the use of cosine normalization . notice the following aspects of the thick line representing pivoted length-normalization : it is linear in the document-length and has the form (31) where is the cosine normalization value at which the two curves intersect . its slope is and (3) it crosses the line at piv . 31 (32) figure 6.17 : implementing pivoted document-length-normalization by linear-scaling . of course , pivoted document-length-normalization is not appropriate for all applications . for instance , in a collection of answers to frequently-asked-questions (say , at a customer-service website) , relevance may have little to do with document-length . in other cases the dependency may be more complex than can be accounted for by a simple linear pivoted normalization . in such cases , document-length can be used as a feature in the machine-learning based scoring approach of section 6.1.2 . exercises . one measure of the similarity of two vectors is the euclidean-distance (or distance) between them : (33) given a query and documents , we may rank the documents in order of increasing euclidean-distance from . show that if and the are all normalized to unit vectors , then the rank ordering produced by euclidean-distance is identical to that produced by cosine similarities . compute the vector-space similarity between the query `` digital-cameras '' and the document `` digital-cameras and video cameras '' by filling out the empty columns in table 6.1 . assume , logarithmic term-weighting (wf columns) for query and document , idf weighting for the query only and cosine normalization for the document only . treat and as a stop word . enter term counts in the tf columns . what is the final similarity-score ? table 6.1 : cosine computation for exercise 6.4.4 . query document word tf wf df idf tf wf digital 10,000 video 100,000 cameras 50,000 show that for the query affection , the relative ordering of the scores of the three documents in figure 6.13 is the reverse of the ordering of the scores for the query jealous gossip . in turning a query into a unit vector in figure 6.13 , we assigned equal weights to each of the query terms . what other principled approaches are plausible ? consider the case of a query-term that is not in the set of indexed terms ; thus our standard construction of the query-vector results in not being in the vector-space created from the collection . how would one adapt the vector-space-representation to handle this case ? refer to the tf and idf values for four terms and three documents in exercise 6.2.2 . compute the two top scoring documents on the query best car-insurance for each of the following weighing schemes : (i) nnn.atc ; (ii) ntc.atc . suppose that the word coyote does not occur in the collection used in exercises 6.2.2 and 6.4.4 . how would one compute ntc.atc scores for the query coyote insurance ?