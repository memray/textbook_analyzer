278 chapter 1 : 11.4 summary-evaluation of information-retrieval-systems is essential to understand the source of weaknesses in existing systems and trade offs between using different algorithms . the standard measures of precision , recall , and fallout have been used for the last twenty-five years as the major measures of algorithmic effectiveness . with the insertion of information-retrieval technologies into the commercial market and ever growing use on the internet , other measures will be needed for real-time monitoring the operations of systems . one example was given in the modifications to the definition of precision when a user ends his retrieval activity as soon as sufficient information is found to satisfy the reason for the search . the measures to date are optimal from a system perspective , and very useful in evaluating the effect of changes to search-algorithms . what are missing are the evaluation-metrics that consider the total information-retrieval-system , attempting to estimate the system 's support for satisfying a search versus how well an algorithm performs . this would require additional estimates of the effectiveness of techniques to generate queries and techniques to review the results of searches . being able to take a system perspective may change the evaluation for a particular aspect of the system . for example , assume information-visualization-techniques are needed to improve the user 's effectiveness in locating needed information . two levels of search-algorithms , one optimized for concept-clustering the other optimized for precision , may be more effective than a single algorithm optimized against a standard precision/recall measure . in all cases , evaluation of information-retrieval-systems will suffer from the subjective nature of information . there is no deterministic methodology for understanding what is relevant to a user 's search . the problems with information discussed in chapter 1 directly affect system-evaluation techniques in chapter 11 . users have trouble in translating their mental perception of information being sought into the written language of a search statement . when facts are needed , users are able to provide a specific relevance-judgment on an item . but when general information is needed , relevancy goes from a classification-process to a continuous function . the current evaluation-metrics require a classification of items into relevant or non-relevant . when forced to make this decision , users have a different threshold . these leads to the suggestion that the existing evaluation formulas could benefit from extension to accommodate a spectrum of values for relevancy of an item versus a binary-classification . but the innate issue of the subjective nature of relevant judgments will still exist , just at a different level . research on information-retrieval suffered for many years from a lack of a large , meaningful test-corpora . the text-retrieval conferences (trecs) , sponsored on a yearly basis , provide a source of a large `` ground-truth '' database of documents , search statements and expected results from searches essential to evaluate algorithms . it also provides a yearly forum where developers of algorithms can share their techniques with their peers . more recently , developers are starting information-system evaluation 279 to combine the best parts of their algorithms with other developers algorithms to produce an improved system .