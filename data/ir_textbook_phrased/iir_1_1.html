an example information-retrieval problem a fat book which many people own is shakespeare 's collected works . suppose you wanted to determine which plays of shakespeare contain the words brutus and caesar and not calpurnia . one way to do that is to start at the beginning and to read-through all the text , noting for each play whether it contains brutus and caesar and excluding it from consideration if it contains calpurnia . the simplest form of document-retrieval is for a computer to do this sort of linear-scan through documents . this process is commonly referred to as grepping through text , after the unix command grep , which performs this process . grepping through text can be a very effective process , especially given the speed of modern computers , and often allows useful possibilities for wildcard pattern-matching through the use of . with modern computers , for simple querying of modest collections (the size of shakespeare 's collected works is a bit under one million words of text in total) , you really need nothing more . but for many purposes , you do need more : to process large document collections quickly . the amount of online data has grown at least as quickly as the speed of computers , and we would now like to be able to search collections that total in the order of billions to trillions of words . to allow more flexible matching operations . for example , it is impractical to perform the query romans near countrymen with grep , where near might be defined as `` within 5 words '' or `` within the same sentence '' . to allow ranked-retrieval : in many cases you want the best answer to an information-need among many documents that contain certain words . the way to avoid linearly scanning the texts for each query is to index the documents in advance . let us stick with shakespeare 's collected works , and use it to introduce the basics of the boolean-retrieval model . suppose we record for each document - here a play of shakespeare 's - whether it contains each word out of all the words shakespeare used (shakespeare used about 32,000 different words) . the result is a binary term-document incidence matrix , as in figure 1.1 . terms are the indexed units (further discussed in section 2.2) ; they are usually words , and for the moment you can think of them as words , but the information-retrieval literature normally speaks of terms because some of them , such as perhaps i-9 or hong-kong are not usually thought of as words . now , depending on whether we look at the-matrix rows or columns , we can have a vector for each term , which shows the documents it appears in , or a vector for each document , showing the terms that occur in it . to answer the query brutus and caesar and not calpurnia , we take the vectors for brutus , caesar and calpurnia , complement the last , and then do a bitwise and : 110100 and 110111 and 101111 = 100100 1.2 the boolean-retrieval model is a model for information-retrieval in which we can pose any query which is in the form of a boolean-expression of terms , that is , in which terms are combined with the operators and , or , and not . the model views each document as just a set of words . figure : results from shakespeare for the query brutus and caesar and not calpurnia . let us now consider a more realistic scenario , simultaneously using the opportunity to introduce some terminology and notation . suppose we have documents . by documents we mean whatever units we have decided to build a retrieval-system over . they might be individual memos or chapters of a book (see section 2.1.2 (page) for further discussion) . we will refer to the group of documents over which we perform retrieval as the (document) collection . it is sometimes also referred to as a corpus (a body of texts) . suppose each document is about 1000 words long (2-3 book pages) . if we assume an average of 6 bytes per word including spaces and punctuation , then this is a document-collection about 6 gb in size . typically , there might be about distinct terms in these documents . there is nothing special about the numbers we have chosen , and they might vary by an order of magnitude or more , but they give us some idea of the dimensions of the kinds of problems we need to handle . we will discuss and model these size assumptions in section 5.1 (page) . our goal is to develop a system to address the ad-hoc-retrieval task . this is the most standard ir task . in it , a system aims to provide documents from within the collection that are relevant to an arbitrary user-information need , communicated to the system by means of a one-off , user-initiated query . an information-need is the topic about which the user desires to know more , and is differentiated from a query , which is what the user conveys to the computer in an attempt to communicate the information-need . a document is relevant if it is one that the user perceives as containing information of value with respect to their personal-information need . our example above was rather artificial in that the information-need was defined in terms of particular words , whereas usually a user is interested in a topic like `` pipeline leaks '' and would like to find relevant documents regardless of whether they precisely use those words or express the concept with other words such as pipeline rupture . to assess the effectiveness of an ir-system (i.e. , the quality of its search-results) , a user will usually want to know two key statistics about the system 's returned results for a query : precision : what fraction of the returned results are relevant to the information-need ? recall : what fraction of the relevant documents in the collection were returned by the system ? 8 we now can not build a term-document-matrix in a naive way . a matrix has half-a-trillion 0 's and 1 's - too many to fit in a computer 's memory . but the crucial observation is that the-matrix is extremely sparse , that is , it has few non-zero entries . because each document is 1000 words long , the-matrix has no more than one billion 1 's , so a minimum of 99.8 % of the cells are zero . a much better representation is to record only the things that do occur , that is , the 1 positions . this idea is central to the first major concept in information-retrieval , the inverted-index . the name is actually redundant : an index always maps back from terms to the parts of a document where they occur . nevertheless , inverted-index , or sometimes inverted-file , has become the standard term in information retrieval.the basic idea of an inverted-index is shown in figure 1.3 . we keep a dictionary of terms (sometimes also referred to as a vocabulary or lexicon ; in this book , we use dictionary for the data-structure and vocabulary for the set of terms) . then for each term , we have a list that records which documents the term occurs in . each item in the list - which records that a term appeared in a document (and , later , often , the positions in the document) - is conventionally called a posting . the list is then called a postings list (or) , and all the postings lists taken together are referred to as the postings . the dictionary in figure 1.3 has been sorted alphabetically and each postings list is sorted by document id . we will see why this is useful in section 1.3 , below , but later we will also consider alternatives to doing this (section 7.1.5) .