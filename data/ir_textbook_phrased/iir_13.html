text-classification and naive-bayes thus far , this book has mainly discussed the process of ad-hoc-retrieval , where users have transient information-needs that they try to address by posing one or more queries to a search-engine . however , many users have ongoing information-needs . for example , you might need to track developments in multicore computer chips . one way of doing this is to issue the query multicore and computer and chip against an index of recent newswire articles each morning . in this and the following two chapters we examine the question : how can this repetitive-task be automated ? to this end , many systems support standing queries . a standing-query is like any other query except that it is periodically executed on a collection to which new documents are incrementally added over time . if your standing-query is just multicore and computer and chip , you will tend to miss many relevant new articles which use other terms such as multicore-processors . to achieve good recall , standing queries thus have to be refined over time and can gradually become quite complex . in this example , using a boolean search-engine with stemming , you might end up with a query like (multicore or multi-core) and (chip or processor or microprocessor) . to capture the generality and scope of the problem-space to which standing queries belong , we now introduce the general notion of a classification-problem . given a set of classes , we seek to determine which class (es) a given object belongs to . in the example , the standing-query serves to divide new newswire articles into the two classes : documents about multicore computer chips and documents not about multicore computer chips . we refer to this as two-class-classification . classification using standing queries is also called routing or filtering and will be discussed further in section 15.3.1 (page) . a class need not be as narrowly focused as the standing-query multicore computer chips . often , a class is a more general subject area like china or coffee . such more general classes are usually referred to as topics , and the classification-task is then called text-classification , text-categorization , topic-classification , or topic-spotting . an example for china appears in figure 13.1 . standing queries and topics differ in their degree of specificity , but the methods for solving routing , filtering , and text-classification are essentially the same . we therefore include routing and filtering under the rubric of text-classification in this and the following chapters . the notion of classification is very general and has many applications within and beyond information-retrieval (ir) . for instance , in computer-vision , a classifier may be used to divide images into classes such as landscape , portrait , and neither . we focus here on examples from information-retrieval such as : several of the preprocessing steps necessary for indexing as discussed in chapter 2 : detecting a document 's encoding (ascii , unicode utf-8 etc ; page 2.1.1) ; word-segmentation (is the white-space between two letters a word-boundary or not ? page 24) ; truecasing (page 2.2.3) ; and identifying the language of a document (page 2.5) . the automatic-detection of spam pages (which then are not included in the search-engine index) . the automatic-detection of sexually explicit content (which is included in search-results only if the user turns an option such as safesearch off) . sentiment-detection or the automatic-classification of a movie or product-review as positive or negative . an example application is a user searching for negative reviews before buying a camera to make sure it has no undesirable features or quality problems . personal email sorting . a user may have folders like talk announcements , electronic bills , email from family-and-friends , and so on , and may want a classifier to classify each incoming email and automatically move it to the appropriate folder . it is easier to find messages in sorted folders than in a very-large inbox . the most common case of this application is a spam folder that holds all suspected spam messages . topic-specific or vertical-search . vertical-search-engines restrict searches to a particular topic . for example , the query computer-science on a vertical-search-engine for the topic china will return a list of chinese computer-science departments with higher precision-and-recall than the query computer-science china on a general purpose search-engine . this is because the vertical-search-engine does not include web-pages in its index that contain the term china in a different sense (e.g. , referring to a hard white ceramic) , but does include relevant pages even if they do not explicitly mention the term china . finally , the ranking-function in ad-hoc-information-retrieval can also be based on a document classifier as we will explain in section 15.4 (page) . this list shows the general importance of classification in ir . most retrieval-systems today contain multiple components that use some form of classifier . the classification-task we will use as an example in this book is text-classification . a computer is not essential for classification . many classification-tasks have traditionally been solved manually . books in a library are assigned library-of-congress categories by a librarian . but manual-classification is expensive to scale . the multicore computer chips example illustrates one alternative approach : classification by the use of standing queries - which can be thought of as rules - most commonly written by hand . as in our example (multicore or multi-core) and (chip or processor or microprocessor) , rules are sometimes equivalent to boolean-expressions . a rule captures a certain combination of keywords that indicates a class . hand-coded rules have good scaling properties , but creating and maintaining them over time is labor intensive . a technically skilled person (e.g. , a domain expert who is good at writing regular-expressions) can create rule-sets that will rival or exceed the accuracy of the automatically generated classifiers we will discuss shortly ; however , it can be hard to find someone with this specialized skill . apart from manual-classification and hand-crafted rules , there is a third approach to text-classification , namely , machine learning-based text-classification . it is the approach that we focus on in the next several chapters . in machine-learning , the set of rules or , more generally , the decision criterion of the text-classifier , is learned automatically from training-data . this approach is also called statistical text classification if the learning-method is statistical . in statistical text classification , we require a number of good example documents (or training-documents) for each class . the need for manual-classification is not eliminated because the training-documents come from a person who has labeled them - where labeling refers to the process of annotating each document with its class . but labeling is arguably an easier task than writing rules . almost anybody can look at a document and decide whether or not it is related to china . sometimes such labeling is already implicitly part of an existing workflow . for instance , you may go through the news-articles returned by a standing-query each morning and give relevance-feedback (cf. chapter 9) by moving the relevant articles to a special folder like multicore-processors . we begin this chapter with a general introduction to the text-classification problem including a formal-definition (section 13.1) ; we then cover naive-bayes , a particularly simple and effective classification-method (sections 13.2-13 .4) . all of the classification algorithms we study represent documents in high-dimensional spaces . to improve the efficiency of these algorithms , it is generally desirable to reduce the dimensionality of these spaces ; to this end , a technique known as feature-selection is commonly applied in text-classification as discussed in section 13.5 . section 13.6 covers evaluation of text-classification . in the following chapters , chapters 14 15 , we look at two other families of classification-methods , vector-space classifiers and support-vector-machines . subsections the text-classification problem naive-bayes text-classification relation to multinomial unigram-language-model the bernoulli model-properties of naive-bayes a variant of the multinomial-model feature-selection mutual-information feature selectionchi2 feature-selection assessing as a feature-selection methodassessing chi-square as a feature-selection-method frequency-based feature-selection feature-selection for multiple-classifiers comparison of feature-selection methods evaluation of text-classification references and further reading