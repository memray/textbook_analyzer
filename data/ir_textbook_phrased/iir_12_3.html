language-modeling versus other approaches in ir the language-modeling-approach provides a novel way of looking at the problem of text-retrieval , which links it with a lot of recent work in speech and language processing . as ponte and croft (1998) emphasize , the language-modeling-approach to ir provides a different approach to scoring matches between queries and documents , and the hope is that the probabilistic-language-modeling foundation improves the weights that are used , and hence the performance of the model . the major issue is estimation of the document model , such as choices of how to smooth it effectively . the model has achieved very good retrieval results . compared to other probabilistic-approaches , such as the bim from chapter 11 , the main difference initially appears to be that the lm approach does away with explicitly modeling relevance (whereas this is the central variable evaluated in the bim approach) . but this may not be the correct way to think about things , as some of the papers in section 12.5 further discuss . the lm approach assumes that documents and expressions of information-needs are objects of the same type , and assesses their match by importing the tools-and-methods of language-modeling from speech and natural language processing . the resulting model is mathematically precise , conceptually simple , computationally tractable , and intuitively appealing . this seems similar to the situation with xml-retrieval (chapter 10) : there the approaches that assume queries and documents are objects of the same type are also among the most successful . on the other hand , like all ir-models , you can also raise objections to the model . the assumption of equivalence between document and information-need representation is unrealistic . current lm approaches use very simple models of language , usually unigram models . without an explicit notion of relevance , relevance-feedback is difficult to integrate into the model , as are user-preferences . it also seems necessary to move beyond a unigram-model to accommodate notions of phrase or passage matching or boolean-retrieval operators . subsequent work in the lm approach has looked at addressing some of these concerns , including putting relevance back into the model and allowing a language mismatch between the query-language and the document language . the model has significant relations to traditional tf-idf models . term-frequency is directly represented in tf-idf models , and much recent work has recognized the importance of document-length-normalization . the effect of doing a mixture of document-generation probability with collection generation probability is a little like idf : terms rare in the general collection but common in some documents will have a greater influence on the ranking of documents . in most concrete realizations , the models share treating terms as if they were independent . on the other hand , the intuitions are probabilistic rather than geometric , the mathematical-models are more principled rather than heuristic , and the details of how statistics like term-frequency and document-length are used differ . if you are concerned mainly with performance numbers , recent work has shown the lm approach to be very effective in retrieval experiments , beating tf-idf and bm25 weights . nevertheless , there is perhaps still insufficient evidence that its performance so greatly exceeds that of a well-tuned traditional vector-space-retrieval system as to justify changing an existing implementation .