obtaining the character-sequence in a document digital-documents that are the input to an indexing process are typically bytes in a file or on a web-server . the first step of processing is to convert this byte sequence into a linear sequence of characters . for the case of plain english-text in ascii encoding , this is trivial . but often things get much more complex . the sequence of characters may be encoded by one of various single byte or multibyte encoding-schemes , such as unicode utf-8 , or various national or vendor-specific standards . we need to determine the correct encoding . this can be regarded as a machine-learning-classification problem , as discussed in chapter 13 , but is often handled by heuristic-methods , user selection , or by using provided document-metadata . once the encoding is determined , we decode the byte sequence to a character-sequence . we might save the choice of encoding because it gives some evidence about what language the document is written in . the characters may have to be decoded out of some binary representation like microsoft-word doc files and/or a compressed format such as zip files . again , we must determine the document format , and then an appropriate decoder has to be used . even for plain-text documents , additional decoding may need to be done . in xml-documents xmlbasic , character entities , such as amp ; , need to be decoded to give the correct character , namely for amp ; . finally , the textual part of the document may need to be extracted out of other material that will not be processed . this might be the desired handling for xml files , if the markup is going to be ignored ; we would almost certainly want to do this with postscript or pdf-files . we will not deal further with these issues in this book , and will assume henceforth that our documents are a list of characters . commercial products usually need to support a broad range of document types and encodings , since users want things to just work with their data as is . often , they just think of documents as text inside applications and are not even aware of how it is encoded on disk . this problem is usually solved by licensing a software-library that handles decoding document-formats and character encodings . the idea that text is a linear sequence of characters is also called into question by some writing systems , such as arabic , where text takes on some two dimensional and mixed order characteristics , as shown in and 2.2 . but , despite some complicated writing-system conventions , there is an underlying sequence of sounds being represented and hence an essentially linear-structure remains , and this is what is represented in the digital representation of arabic , as shown in figure 2.1 . an example of a vocalized modern-standard-arabic word.the writing is from right to left and letters undergo complex mutations as they are combined . the representation of short vowels (here , / i / and / u /) and the final / n / (nunation) departs from strict linearity by being represented as diacritics above and below letters . nevertheless , the represented text is still clearly a linear ordering of characters representing sounds . full vocalization , as here , normally appears only in the koran and children 's books . day-to-day text is unvocalized (short vowels are not represented but the letter for a would still appear) or partially vocalized , with short vowels inserted in places where the writer perceives ambiguities . these choices add further complexities to indexing . the conceptual linear-order of characters is not necessarily the order that you see on the page . in languages that are written right-to-left , such as hebrew and arabic , it is quite common to also have left-to-right text interspersed , such as numbers and dollar amounts . with modern unicode representation concepts , the order of characters in files matches the conceptual order , and the reversal of displayed characters is handled by the rendering-system , but this may not be true for documents in older encodings .